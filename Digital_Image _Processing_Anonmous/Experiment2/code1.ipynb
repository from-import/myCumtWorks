{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fe8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7888711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数定义\n",
    "#dataset = \"FashionMNIST\" #选择所使用的数据集\"MNIST\"or\"FashionMNIST\"\n",
    "dataset = \"MNIST\"\n",
    "Net = \"LeNet5\" #选择所使用的网络\"LeNet5\"or \"LinearNet\"\n",
    "# Net = \"LinearNet\" #选择所使用的网络\"LeNet5\"or \"LinearNet\"\n",
    "batch_size = 64 #定义批处理大小\n",
    "lr=0.01 #设置learning rate学习率\n",
    "epochs = 8 #指定训练迭代次数\n",
    "save_path = \"./\" #模型保存路径\n",
    "Early_Stopping = 0 #选择是否使用Early Stopping训练模式，训练时根据精度的变化率来控制训练迭代代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a01ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载数据集\n",
    "if dataset==\"MNIST\":\n",
    "    # 从torchvision下载训练集.\n",
    "    training_data = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # #从torchvision下载测试集.\n",
    "    test_data = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    # 若是数字数据集则设置成下述类别\n",
    "    classes = [ \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "else:\n",
    "    # 从torchvision下载训练集.\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # #从torchvision下载测试集.\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    # 若是服饰数据集则设置成下述类别\n",
    "    classes = [ \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\",\"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "905c5adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建数据加载器.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "#观察数据样本\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    fig = plt.figure()\n",
    "    # 对部分数据进行显示\n",
    "    for i in range(6):\n",
    "      plt.subplot(2,3,i+1)\n",
    "      plt.tight_layout()\n",
    "      plt.imshow(X[i][0], cmap='gray', interpolation='none')\n",
    "      plt.title(\"Ground Truth: {}\".format(classes[int(y[i])]))\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08974297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# 自动选择cpu或gpu用于训练.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c7b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    残差块定义：\n",
    "    参数：\n",
    "        in_channels：输入特征的通道数\n",
    "        out_channels: 输出特征的通道数\n",
    "        stride: 步长\n",
    "        downsample: 如果输入输出不匹配，用于下采样的模块\n",
    "'''\n",
    "\n",
    "class ResidualBlock(nn.Module):  \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):  \n",
    "        super(ResidualBlock, self).__init__()  \n",
    "            \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) \n",
    "        \n",
    "        # 进行批量归一化\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU(inplace=True)  \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)  \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  \n",
    "          \n",
    "        # 短路连接（如果输入输出通道数或特征图大小不同，需要进行下采样）  \n",
    "        self.downsample = downsample  \n",
    "          \n",
    "    def forward(self, x):  \n",
    "        residual = x\n",
    "          \n",
    "        # 主路径\n",
    "#         print(x.shape)\n",
    "        out = self.conv1(x)  \n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)  \n",
    "        out = self.conv2(out)  \n",
    "        out = self.bn2(out)  \n",
    "          \n",
    "        # 短路连接  \n",
    "        if self.downsample is not None:  \n",
    "            residual = self.downsample(x)  \n",
    "          \n",
    "        # 残差连接  \n",
    "        # 将主路径和短路连接的结果相加\n",
    "        out += residual  \n",
    "        out = self.relu(out)  \n",
    "        \n",
    "        # 返回经过残差处理后的特征图\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274dd484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1(\n",
      "  (conv1): Conv2d(1, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2d): Sequential(\n",
      "    (0): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res_block): ResidualBlock(\n",
      "    (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(10, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out1): Linear(in_features=261, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#定义模型\n",
    "\n",
    "if Net == \"LinearNet\":\n",
    "    # 普通神经网络\n",
    "    class LinearNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(28*28, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 10)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            logits = self.linear_relu_stack(x)\n",
    "            return logits\n",
    "\n",
    "    model = LinearNet().to(device)\n",
    "    \n",
    "else:\n",
    "    class CNN_1(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_1, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                    ## 卷积核大小为羽3，输入通道为__，输出通道为13，步长为1;\n",
    "                    ## paddingHzero padding\n",
    "                    ##要求经过conv1的输出空间维度与输入的空间维度相同\n",
    "                    in_channels = 1,\n",
    "                    out_channels = 13,\n",
    "                    kernel_size = 3,\n",
    "                    stride= 1,\n",
    "                    padding= 1,\n",
    "                )\n",
    "            ##激活函数+最大池化\n",
    "            self.relu = nn.ReLU()\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "            self.conv2d = nn.Sequential(\n",
    "            ## 1. 卷积核大小为3*3，输入通道为10，输出通道为10，padding 方法为same，padding大小为？？？步长为??\n",
    "            ## 2.自行选择激活函数\n",
    "            ## 3. 池化\n",
    "                nn.Conv2d(in_channels=13,\n",
    "                         out_channels=10,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1),\n",
    "                self.relu,\n",
    "                self.pool\n",
    "            )\n",
    "            ## 添加残差连接模块\n",
    "            #############残差模块设计部分################\n",
    "            self.res_block = ResidualBlock(10, 10)\n",
    "\n",
    "            ############################################\n",
    "            self.conv3 = nn.Sequential(\n",
    "                ##自行设计卷积模块\n",
    "                nn.Conv2d(in_channels=10,\n",
    "                         out_channels=29,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1),\n",
    "                self.relu,\n",
    "                self.pool\n",
    "            )\n",
    "            self.out1 = nn.Linear(3*3*29, 10, bias = True)\n",
    "            ## 在下方添加Dropout以及其他代码\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            ## 请将余下代码补充完整\n",
    "            # 卷积后图像大小保持不变\n",
    "            # (64,1,28,28)\n",
    "            x = self.relu(self.conv1(x))\n",
    "            x = self.pool(x)\n",
    "            # (64,13,14,14)\n",
    "            x = self.conv2d(x)\n",
    "            # (64,10,7,7)\n",
    "            x = self.res_block(x)\n",
    "            # (64,10,7,7)\n",
    "            x = self.conv3(x)\n",
    "            # (64,10,3,3)\n",
    "            \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.out1(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "\n",
    "    model = CNN_1().to(device)\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2c81349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了训练模型，我们需要一个损失函数和一个优化器\n",
    "loss_fn = nn.CrossEntropyLoss()#定义损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "\n",
    "#在单个训练循环中，模型对训练数据集进行预测（分批提供给它），并反向传播预测误差以调整模型的参数\n",
    "def train(dataloader, model, loss_fn, optimizer, flag=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        if flag == 1:\n",
    "            tensor_new = torch.zeros(y.shape[0], 10)\n",
    "            for i in range(len(y)):\n",
    "                tensor_new[i,y[i]] = 1\n",
    "        # Compute prediction error\n",
    "        tensor_new = tensor_new.to(device)\n",
    "        pred = model(X)\n",
    "#         print(pred.shape)\n",
    "#         print(tensor_new.shape)\n",
    "        loss = loss_fn(pred, tensor_new)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "#对照测试数据集检查模型的性能，以确保它正在学习\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            if flag == 1:\n",
    "                tensor_new = torch.zeros(y.shape[0], 10)\n",
    "                for i in range(len(y)):\n",
    "                    tensor_new[i,y[i]] = 1\n",
    "            tensor_new = tensor_new.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, tensor_new).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n",
    "#实现Early Stopping训练\n",
    "class EarlyStopping:\n",
    "    def __init__(self, save_path, patience=2, verbose=False, delta=0.03):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            save_path : 模型保存路径\n",
    "            patience (int): 设置将连续几次训练迭代纳入Early Stopping考评\n",
    "            verbose (bool): 如果是 \"True\"，则为每次验证损失的优化值打印一条信息\n",
    "            delta (float): 前后两次训练迭代的最小变化阈值，小于该阈值则认为模型优化幅度有限，将该次迭代计入patience，\n",
    "                           数量达到patience则提前停止训练。\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        # 尚未找到最佳的验证分数\n",
    "        if self.best_score is None:\n",
    "            # 将当前的分数作为最佳分数\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        # 如果分数没有得到明显改善\n",
    "        elif score < self.best_score + self.delta:\n",
    "            # 计时器加1操作\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # 如果超出一定时间，说明达到最优，则进行保存\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # 如果得到明显改善，说明还未达到最优，则进行存储，同时继续进行查找\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''当验证损失降低时，保存模型'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        path = os.path.join(self.save_path, 'best_network.pth')\n",
    "        torch.save(model.state_dict(), path)\t# 这里会存储迄今最优模型的参数\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3024dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.413957  [   64/60000]\n",
      "loss: 2.270190  [ 6464/60000]\n",
      "loss: 2.241194  [12864/60000]\n",
      "loss: 2.230300  [19264/60000]\n",
      "loss: 2.259474  [25664/60000]\n",
      "loss: 2.166474  [32064/60000]\n",
      "loss: 2.085890  [38464/60000]\n",
      "loss: 2.059524  [44864/60000]\n",
      "loss: 2.128822  [51264/60000]\n",
      "loss: 1.998722  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.903534 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.924064  [   64/60000]\n",
      "loss: 1.853441  [ 6464/60000]\n",
      "loss: 1.672357  [12864/60000]\n",
      "loss: 1.743059  [19264/60000]\n",
      "loss: 1.678233  [25664/60000]\n",
      "loss: 1.619336  [32064/60000]\n",
      "loss: 1.360625  [38464/60000]\n",
      "loss: 1.513050  [44864/60000]\n",
      "loss: 1.445834  [51264/60000]\n",
      "loss: 1.111641  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.3%, Avg loss: 1.073851 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.258661  [   64/60000]\n",
      "loss: 1.185120  [ 6464/60000]\n",
      "loss: 0.947732  [12864/60000]\n",
      "loss: 1.021074  [19264/60000]\n",
      "loss: 0.962733  [25664/60000]\n",
      "loss: 0.889788  [32064/60000]\n",
      "loss: 0.773973  [38464/60000]\n",
      "loss: 0.801213  [44864/60000]\n",
      "loss: 0.941816  [51264/60000]\n",
      "loss: 0.783696  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.576440 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.719163  [   64/60000]\n",
      "loss: 0.693918  [ 6464/60000]\n",
      "loss: 0.473295  [12864/60000]\n",
      "loss: 0.768937  [19264/60000]\n",
      "loss: 0.528855  [25664/60000]\n",
      "loss: 0.599252  [32064/60000]\n",
      "loss: 0.499470  [38464/60000]\n",
      "loss: 0.516844  [44864/60000]\n",
      "loss: 0.709187  [51264/60000]\n",
      "loss: 0.548711  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.362493 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.542298  [   64/60000]\n",
      "loss: 0.366624  [ 6464/60000]\n",
      "loss: 0.373248  [12864/60000]\n",
      "loss: 0.580419  [19264/60000]\n",
      "loss: 0.357992  [25664/60000]\n",
      "loss: 0.328950  [32064/60000]\n",
      "loss: 0.376547  [38464/60000]\n",
      "loss: 0.403850  [44864/60000]\n",
      "loss: 0.555270  [51264/60000]\n",
      "loss: 0.355973  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.4%, Avg loss: 0.266585 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.328804  [   64/60000]\n",
      "loss: 0.275637  [ 6464/60000]\n",
      "loss: 0.287755  [12864/60000]\n",
      "loss: 0.438233  [19264/60000]\n",
      "loss: 0.304786  [25664/60000]\n",
      "loss: 0.288678  [32064/60000]\n",
      "loss: 0.342926  [38464/60000]\n",
      "loss: 0.274848  [44864/60000]\n",
      "loss: 0.442420  [51264/60000]\n",
      "loss: 0.307356  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.231048 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.360475  [   64/60000]\n",
      "loss: 0.201051  [ 6464/60000]\n",
      "loss: 0.320166  [12864/60000]\n",
      "loss: 0.406320  [19264/60000]\n",
      "loss: 0.305194  [25664/60000]\n",
      "loss: 0.285950  [32064/60000]\n",
      "loss: 0.330263  [38464/60000]\n",
      "loss: 0.222626  [44864/60000]\n",
      "loss: 0.474902  [51264/60000]\n",
      "loss: 0.382957  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.183522 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.322817  [   64/60000]\n",
      "loss: 0.263782  [ 6464/60000]\n",
      "loss: 0.194433  [12864/60000]\n",
      "loss: 0.391371  [19264/60000]\n",
      "loss: 0.211199  [25664/60000]\n",
      "loss: 0.226245  [32064/60000]\n",
      "loss: 0.173773  [38464/60000]\n",
      "loss: 0.267359  [44864/60000]\n",
      "loss: 0.420761  [51264/60000]\n",
      "loss: 0.248640  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.8%, Avg loss: 0.186013 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#训练过程是在所定义的几个迭代上进行的。在每次迭代，模型学习参数以做出更好的预测。我们在每次迭代打印模型的准确性和损失；我们希望看到精度随着迭代次数的增加而增加，而损失随着迭代次数的减少而减少\n",
    "early_stopping = EarlyStopping(save_path)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss = test(test_dataloader, model, loss_fn)\n",
    "    if Early_Stopping:\n",
    "        early_stopping(test_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break #跳出迭代，结束训练\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "769df87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "#保存模型的一种常见方法是序列化内部状态字典(包含模型参数)\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ef49333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 7, 7])\n",
      "torch.Size([1, 10, 7, 7])\n",
      "Predicted: \"7\", Actual: \"7\"\n"
     ]
    }
   ],
   "source": [
    "#加载模型的过程包括重新创建模型结构并将状态字典加载到其中。\n",
    "model = LinearNet().to(device) if Net == \"LinearNet\" else CNN_1().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.unsqueeze_(0).to(device)\n",
    "#     x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f885759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.337886  [   64/60000]\n",
      "loss: 2.275381  [ 6464/60000]\n",
      "loss: 2.128357  [12864/60000]\n",
      "loss: 2.094188  [19264/60000]\n",
      "loss: 2.077389  [25664/60000]\n",
      "loss: 2.016683  [32064/60000]\n",
      "loss: 1.834810  [38464/60000]\n",
      "loss: 1.831442  [44864/60000]\n",
      "loss: 1.813053  [51264/60000]\n",
      "loss: 1.557953  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.9%, Avg loss: 1.499821 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.608020  [   64/60000]\n",
      "loss: 1.464917  [ 6464/60000]\n",
      "loss: 1.351208  [12864/60000]\n",
      "loss: 1.427811  [19264/60000]\n",
      "loss: 1.228829  [25664/60000]\n",
      "loss: 1.237968  [32064/60000]\n",
      "loss: 0.976758  [38464/60000]\n",
      "loss: 1.037702  [44864/60000]\n",
      "loss: 1.128720  [51264/60000]\n",
      "loss: 0.865112  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.748197 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.877750  [   64/60000]\n",
      "loss: 0.760483  [ 6464/60000]\n",
      "loss: 0.700797  [12864/60000]\n",
      "loss: 0.831600  [19264/60000]\n",
      "loss: 0.721596  [25664/60000]\n",
      "loss: 0.705460  [32064/60000]\n",
      "loss: 0.596890  [38464/60000]\n",
      "loss: 0.712232  [44864/60000]\n",
      "loss: 0.803299  [51264/60000]\n",
      "loss: 0.652920  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.441094 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.609548  [   64/60000]\n",
      "loss: 0.481708  [ 6464/60000]\n",
      "loss: 0.404396  [12864/60000]\n",
      "loss: 0.597941  [19264/60000]\n",
      "loss: 0.463432  [25664/60000]\n",
      "loss: 0.446228  [32064/60000]\n",
      "loss: 0.484113  [38464/60000]\n",
      "loss: 0.422146  [44864/60000]\n",
      "loss: 0.500687  [51264/60000]\n",
      "loss: 0.463920  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.309251 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.495449  [   64/60000]\n",
      "loss: 0.359001  [ 6464/60000]\n",
      "loss: 0.394037  [12864/60000]\n",
      "loss: 0.386507  [19264/60000]\n",
      "loss: 0.445414  [25664/60000]\n",
      "loss: 0.417104  [32064/60000]\n",
      "loss: 0.419625  [38464/60000]\n",
      "loss: 0.361689  [44864/60000]\n",
      "loss: 0.514566  [51264/60000]\n",
      "loss: 0.425587  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.8%, Avg loss: 0.243679 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.424237  [   64/60000]\n",
      "loss: 0.383063  [ 6464/60000]\n",
      "loss: 0.190323  [12864/60000]\n",
      "loss: 0.419770  [19264/60000]\n",
      "loss: 0.312698  [25664/60000]\n",
      "loss: 0.220459  [32064/60000]\n",
      "loss: 0.327489  [38464/60000]\n",
      "loss: 0.413518  [44864/60000]\n",
      "loss: 0.362128  [51264/60000]\n",
      "loss: 0.321810  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.199099 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.319278  [   64/60000]\n",
      "loss: 0.237661  [ 6464/60000]\n",
      "loss: 0.180013  [12864/60000]\n",
      "loss: 0.335575  [19264/60000]\n",
      "loss: 0.261022  [25664/60000]\n",
      "loss: 0.229051  [32064/60000]\n",
      "loss: 0.241793  [38464/60000]\n",
      "loss: 0.281496  [44864/60000]\n",
      "loss: 0.370292  [51264/60000]\n",
      "loss: 0.331595  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.171995 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.219669  [   64/60000]\n",
      "loss: 0.169275  [ 6464/60000]\n",
      "loss: 0.141220  [12864/60000]\n",
      "loss: 0.312850  [19264/60000]\n",
      "loss: 0.226638  [25664/60000]\n",
      "loss: 0.187120  [32064/60000]\n",
      "loss: 0.204255  [38464/60000]\n",
      "loss: 0.305092  [44864/60000]\n",
      "loss: 0.400868  [51264/60000]\n",
      "loss: 0.215826  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.152677 \n",
      "\n",
      "Done!\n",
      "lr: 0.01\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.347227  [   64/60000]\n",
      "loss: 1.721908  [ 6464/60000]\n",
      "loss: 0.924121  [12864/60000]\n",
      "loss: 0.727787  [19264/60000]\n",
      "loss: 0.498698  [25664/60000]\n",
      "loss: 0.441648  [32064/60000]\n",
      "loss: 0.341653  [38464/60000]\n",
      "loss: 0.243966  [44864/60000]\n",
      "loss: 0.294120  [51264/60000]\n",
      "loss: 0.307252  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.219675 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.285710  [   64/60000]\n",
      "loss: 0.212718  [ 6464/60000]\n",
      "loss: 0.107533  [12864/60000]\n",
      "loss: 0.241464  [19264/60000]\n",
      "loss: 0.115196  [25664/60000]\n",
      "loss: 0.159516  [32064/60000]\n",
      "loss: 0.205538  [38464/60000]\n",
      "loss: 0.162984  [44864/60000]\n",
      "loss: 0.297871  [51264/60000]\n",
      "loss: 0.180685  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.0%, Avg loss: 0.127638 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.145899  [   64/60000]\n",
      "loss: 0.176121  [ 6464/60000]\n",
      "loss: 0.108379  [12864/60000]\n",
      "loss: 0.139221  [19264/60000]\n",
      "loss: 0.097259  [25664/60000]\n",
      "loss: 0.083829  [32064/60000]\n",
      "loss: 0.096260  [38464/60000]\n",
      "loss: 0.113032  [44864/60000]\n",
      "loss: 0.212524  [51264/60000]\n",
      "loss: 0.130279  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.082898 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.127105  [   64/60000]\n",
      "loss: 0.214008  [ 6464/60000]\n",
      "loss: 0.122042  [12864/60000]\n",
      "loss: 0.151007  [19264/60000]\n",
      "loss: 0.058058  [25664/60000]\n",
      "loss: 0.124795  [32064/60000]\n",
      "loss: 0.122480  [38464/60000]\n",
      "loss: 0.061825  [44864/60000]\n",
      "loss: 0.200667  [51264/60000]\n",
      "loss: 0.105127  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.067056 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.139067  [   64/60000]\n",
      "loss: 0.218360  [ 6464/60000]\n",
      "loss: 0.090874  [12864/60000]\n",
      "loss: 0.092636  [19264/60000]\n",
      "loss: 0.073816  [25664/60000]\n",
      "loss: 0.101097  [32064/60000]\n",
      "loss: 0.100054  [38464/60000]\n",
      "loss: 0.069564  [44864/60000]\n",
      "loss: 0.193529  [51264/60000]\n",
      "loss: 0.121537  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.067060 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.059684  [   64/60000]\n",
      "loss: 0.209668  [ 6464/60000]\n",
      "loss: 0.099302  [12864/60000]\n",
      "loss: 0.063073  [19264/60000]\n",
      "loss: 0.063997  [25664/60000]\n",
      "loss: 0.121614  [32064/60000]\n",
      "loss: 0.041090  [38464/60000]\n",
      "loss: 0.128761  [44864/60000]\n",
      "loss: 0.220874  [51264/60000]\n",
      "loss: 0.033049  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.054147 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.089927  [   64/60000]\n",
      "loss: 0.158425  [ 6464/60000]\n",
      "loss: 0.056820  [12864/60000]\n",
      "loss: 0.041791  [19264/60000]\n",
      "loss: 0.035214  [25664/60000]\n",
      "loss: 0.070662  [32064/60000]\n",
      "loss: 0.060549  [38464/60000]\n",
      "loss: 0.153721  [44864/60000]\n",
      "loss: 0.188417  [51264/60000]\n",
      "loss: 0.103022  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.056267 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.077971  [   64/60000]\n",
      "loss: 0.110124  [ 6464/60000]\n",
      "loss: 0.073291  [12864/60000]\n",
      "loss: 0.111845  [19264/60000]\n",
      "loss: 0.077661  [25664/60000]\n",
      "loss: 0.086158  [32064/60000]\n",
      "loss: 0.024359  [38464/60000]\n",
      "loss: 0.102087  [44864/60000]\n",
      "loss: 0.157747  [51264/60000]\n",
      "loss: 0.090926  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.070720 \n",
      "\n",
      "Done!\n",
      "lr: 0.1\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.322799  [   64/60000]\n",
      "loss: 0.301535  [ 6464/60000]\n",
      "loss: 0.180615  [12864/60000]\n",
      "loss: 0.253303  [19264/60000]\n",
      "loss: 0.141852  [25664/60000]\n",
      "loss: 0.146574  [32064/60000]\n",
      "loss: 0.103907  [38464/60000]\n",
      "loss: 0.174648  [44864/60000]\n",
      "loss: 0.189121  [51264/60000]\n",
      "loss: 0.095757  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.077442 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.054720  [   64/60000]\n",
      "loss: 0.194278  [ 6464/60000]\n",
      "loss: 0.092796  [12864/60000]\n",
      "loss: 0.166487  [19264/60000]\n",
      "loss: 0.102754  [25664/60000]\n",
      "loss: 0.061358  [32064/60000]\n",
      "loss: 0.088148  [38464/60000]\n",
      "loss: 0.053839  [44864/60000]\n",
      "loss: 0.126831  [51264/60000]\n",
      "loss: 0.066550  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.051761 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.039554  [   64/60000]\n",
      "loss: 0.107456  [ 6464/60000]\n",
      "loss: 0.110579  [12864/60000]\n",
      "loss: 0.117350  [19264/60000]\n",
      "loss: 0.030289  [25664/60000]\n",
      "loss: 0.041418  [32064/60000]\n",
      "loss: 0.053736  [38464/60000]\n",
      "loss: 0.096151  [44864/60000]\n",
      "loss: 0.140882  [51264/60000]\n",
      "loss: 0.090323  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.049250 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.050930  [   64/60000]\n",
      "loss: 0.041604  [ 6464/60000]\n",
      "loss: 0.105002  [12864/60000]\n",
      "loss: 0.088428  [19264/60000]\n",
      "loss: 0.118572  [25664/60000]\n",
      "loss: 0.042417  [32064/60000]\n",
      "loss: 0.073549  [38464/60000]\n",
      "loss: 0.047869  [44864/60000]\n",
      "loss: 0.057122  [51264/60000]\n",
      "loss: 0.016865  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.037887 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.019067  [   64/60000]\n",
      "loss: 0.139477  [ 6464/60000]\n",
      "loss: 0.052324  [12864/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.060924  [19264/60000]\n",
      "loss: 0.163670  [25664/60000]\n",
      "loss: 0.052089  [32064/60000]\n",
      "loss: 0.042650  [38464/60000]\n",
      "loss: 0.018096  [44864/60000]\n",
      "loss: 0.162636  [51264/60000]\n",
      "loss: 0.077004  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.035861 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.026944  [   64/60000]\n",
      "loss: 0.082926  [ 6464/60000]\n",
      "loss: 0.071074  [12864/60000]\n",
      "loss: 0.053368  [19264/60000]\n",
      "loss: 0.053900  [25664/60000]\n",
      "loss: 0.020155  [32064/60000]\n",
      "loss: 0.039568  [38464/60000]\n",
      "loss: 0.067532  [44864/60000]\n",
      "loss: 0.138674  [51264/60000]\n",
      "loss: 0.052007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031139 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.047565  [   64/60000]\n",
      "loss: 0.106341  [ 6464/60000]\n",
      "loss: 0.091592  [12864/60000]\n",
      "loss: 0.016693  [19264/60000]\n",
      "loss: 0.083423  [25664/60000]\n",
      "loss: 0.023511  [32064/60000]\n",
      "loss: 0.036530  [38464/60000]\n",
      "loss: 0.010794  [44864/60000]\n",
      "loss: 0.079775  [51264/60000]\n",
      "loss: 0.041858  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.036126 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.013662  [   64/60000]\n",
      "loss: 0.028666  [ 6464/60000]\n",
      "loss: 0.007336  [12864/60000]\n",
      "loss: 0.037214  [19264/60000]\n",
      "loss: 0.034988  [25664/60000]\n",
      "loss: 0.009415  [32064/60000]\n",
      "loss: 0.071400  [38464/60000]\n",
      "loss: 0.013959  [44864/60000]\n",
      "loss: 0.084374  [51264/60000]\n",
      "loss: 0.039627  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.035310 \n",
      "\n",
      "Done!\n",
      "lr: 1\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.370279  [   64/60000]\n",
      "loss: 2.295936  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Done!\n",
      "lr: 10\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.342704  [   64/60000]\n",
      "loss: 2.352186  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.412004  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401534  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.412004  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.412004  [   64/60000]\n",
      "loss: 2.352186  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352186  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401534  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.412004  [   64/60000]\n",
      "loss: 2.352186  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352186  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401534  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同lr的影响\n",
    "lrList = [0.001,0.01,0.1,1,10]\n",
    "lr_loss = []\n",
    "\n",
    "for lr in lrList:\n",
    "    model = CNN_1().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)#定义优化器\n",
    "    print(f\"lr: {lr}\\n-------------------------------------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    lr_loss.append(test_loss)\n",
    "    print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afbab952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barch: 16\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.542672  [   16/60000]\n",
      "loss: 2.395354  [ 1616/60000]\n",
      "loss: 2.340467  [ 3216/60000]\n",
      "loss: 2.291374  [ 4816/60000]\n",
      "loss: 2.108991  [ 6416/60000]\n",
      "loss: 2.226058  [ 8016/60000]\n",
      "loss: 2.023617  [ 9616/60000]\n",
      "loss: 1.913575  [11216/60000]\n",
      "loss: 1.819007  [12816/60000]\n",
      "loss: 1.837480  [14416/60000]\n",
      "loss: 1.710052  [16016/60000]\n",
      "loss: 1.665875  [17616/60000]\n",
      "loss: 1.706272  [19216/60000]\n",
      "loss: 1.324015  [20816/60000]\n",
      "loss: 1.185527  [22416/60000]\n",
      "loss: 1.274170  [24016/60000]\n",
      "loss: 0.987261  [25616/60000]\n",
      "loss: 1.142904  [27216/60000]\n",
      "loss: 0.917135  [28816/60000]\n",
      "loss: 1.222240  [30416/60000]\n",
      "loss: 0.833632  [32016/60000]\n",
      "loss: 0.636076  [33616/60000]\n",
      "loss: 0.924880  [35216/60000]\n",
      "loss: 1.275768  [36816/60000]\n",
      "loss: 0.822202  [38416/60000]\n",
      "loss: 0.555247  [40016/60000]\n",
      "loss: 0.595882  [41616/60000]\n",
      "loss: 0.879492  [43216/60000]\n",
      "loss: 0.584411  [44816/60000]\n",
      "loss: 0.793639  [46416/60000]\n",
      "loss: 0.699987  [48016/60000]\n",
      "loss: 0.339902  [49616/60000]\n",
      "loss: 0.479989  [51216/60000]\n",
      "loss: 0.998198  [52816/60000]\n",
      "loss: 0.759016  [54416/60000]\n",
      "loss: 0.654220  [56016/60000]\n",
      "loss: 0.425383  [57616/60000]\n",
      "loss: 0.315386  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.308823 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.328982  [   16/60000]\n",
      "loss: 0.682130  [ 1616/60000]\n",
      "loss: 0.324215  [ 3216/60000]\n",
      "loss: 0.543862  [ 4816/60000]\n",
      "loss: 0.243515  [ 6416/60000]\n",
      "loss: 0.576643  [ 8016/60000]\n",
      "loss: 0.723375  [ 9616/60000]\n",
      "loss: 1.008246  [11216/60000]\n",
      "loss: 0.385187  [12816/60000]\n",
      "loss: 0.535450  [14416/60000]\n",
      "loss: 0.406735  [16016/60000]\n",
      "loss: 0.377958  [17616/60000]\n",
      "loss: 0.497023  [19216/60000]\n",
      "loss: 0.129757  [20816/60000]\n",
      "loss: 0.237139  [22416/60000]\n",
      "loss: 0.353985  [24016/60000]\n",
      "loss: 0.361787  [25616/60000]\n",
      "loss: 0.542293  [27216/60000]\n",
      "loss: 0.346629  [28816/60000]\n",
      "loss: 0.359675  [30416/60000]\n",
      "loss: 0.180483  [32016/60000]\n",
      "loss: 0.297714  [33616/60000]\n",
      "loss: 0.295775  [35216/60000]\n",
      "loss: 0.687031  [36816/60000]\n",
      "loss: 0.202506  [38416/60000]\n",
      "loss: 0.258593  [40016/60000]\n",
      "loss: 0.406103  [41616/60000]\n",
      "loss: 0.673041  [43216/60000]\n",
      "loss: 0.130870  [44816/60000]\n",
      "loss: 0.445581  [46416/60000]\n",
      "loss: 0.552112  [48016/60000]\n",
      "loss: 0.113461  [49616/60000]\n",
      "loss: 0.174438  [51216/60000]\n",
      "loss: 0.863996  [52816/60000]\n",
      "loss: 0.407616  [54416/60000]\n",
      "loss: 0.182694  [56016/60000]\n",
      "loss: 0.123503  [57616/60000]\n",
      "loss: 0.027975  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.154161 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.178461  [   16/60000]\n",
      "loss: 0.586007  [ 1616/60000]\n",
      "loss: 0.196943  [ 3216/60000]\n",
      "loss: 0.550430  [ 4816/60000]\n",
      "loss: 0.106209  [ 6416/60000]\n",
      "loss: 0.504372  [ 8016/60000]\n",
      "loss: 0.467216  [ 9616/60000]\n",
      "loss: 0.404690  [11216/60000]\n",
      "loss: 0.053119  [12816/60000]\n",
      "loss: 0.232252  [14416/60000]\n",
      "loss: 0.263193  [16016/60000]\n",
      "loss: 0.208008  [17616/60000]\n",
      "loss: 0.317087  [19216/60000]\n",
      "loss: 0.064971  [20816/60000]\n",
      "loss: 0.149243  [22416/60000]\n",
      "loss: 0.257308  [24016/60000]\n",
      "loss: 0.103769  [25616/60000]\n",
      "loss: 0.315957  [27216/60000]\n",
      "loss: 0.317266  [28816/60000]\n",
      "loss: 0.087833  [30416/60000]\n",
      "loss: 0.222803  [32016/60000]\n",
      "loss: 0.086124  [33616/60000]\n",
      "loss: 0.212128  [35216/60000]\n",
      "loss: 0.468328  [36816/60000]\n",
      "loss: 0.216996  [38416/60000]\n",
      "loss: 0.192881  [40016/60000]\n",
      "loss: 0.342427  [41616/60000]\n",
      "loss: 0.223555  [43216/60000]\n",
      "loss: 0.037205  [44816/60000]\n",
      "loss: 0.264005  [46416/60000]\n",
      "loss: 0.269791  [48016/60000]\n",
      "loss: 0.080181  [49616/60000]\n",
      "loss: 0.119349  [51216/60000]\n",
      "loss: 0.319748  [52816/60000]\n",
      "loss: 0.357974  [54416/60000]\n",
      "loss: 0.373369  [56016/60000]\n",
      "loss: 0.097560  [57616/60000]\n",
      "loss: 0.086950  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.120153 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.033284  [   16/60000]\n",
      "loss: 0.555965  [ 1616/60000]\n",
      "loss: 0.278482  [ 3216/60000]\n",
      "loss: 0.405570  [ 4816/60000]\n",
      "loss: 0.027165  [ 6416/60000]\n",
      "loss: 0.175076  [ 8016/60000]\n",
      "loss: 0.267256  [ 9616/60000]\n",
      "loss: 0.516608  [11216/60000]\n",
      "loss: 0.039343  [12816/60000]\n",
      "loss: 0.129221  [14416/60000]\n",
      "loss: 0.114299  [16016/60000]\n",
      "loss: 0.122934  [17616/60000]\n",
      "loss: 0.102605  [19216/60000]\n",
      "loss: 0.036221  [20816/60000]\n",
      "loss: 0.114651  [22416/60000]\n",
      "loss: 0.158649  [24016/60000]\n",
      "loss: 0.346351  [25616/60000]\n",
      "loss: 0.179137  [27216/60000]\n",
      "loss: 0.325781  [28816/60000]\n",
      "loss: 0.094407  [30416/60000]\n",
      "loss: 0.094573  [32016/60000]\n",
      "loss: 0.062427  [33616/60000]\n",
      "loss: 0.253448  [35216/60000]\n",
      "loss: 0.352453  [36816/60000]\n",
      "loss: 0.122670  [38416/60000]\n",
      "loss: 0.326518  [40016/60000]\n",
      "loss: 0.458195  [41616/60000]\n",
      "loss: 0.280703  [43216/60000]\n",
      "loss: 0.080247  [44816/60000]\n",
      "loss: 0.636250  [46416/60000]\n",
      "loss: 0.098929  [48016/60000]\n",
      "loss: 0.031003  [49616/60000]\n",
      "loss: 0.073967  [51216/60000]\n",
      "loss: 0.732964  [52816/60000]\n",
      "loss: 0.127730  [54416/60000]\n",
      "loss: 0.142096  [56016/60000]\n",
      "loss: 0.148904  [57616/60000]\n",
      "loss: 0.019702  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.3%, Avg loss: 0.089542 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.017371  [   16/60000]\n",
      "loss: 0.511832  [ 1616/60000]\n",
      "loss: 0.086481  [ 3216/60000]\n",
      "loss: 0.175784  [ 4816/60000]\n",
      "loss: 0.032685  [ 6416/60000]\n",
      "loss: 0.248566  [ 8016/60000]\n",
      "loss: 0.319111  [ 9616/60000]\n",
      "loss: 0.220091  [11216/60000]\n",
      "loss: 0.053094  [12816/60000]\n",
      "loss: 0.120712  [14416/60000]\n",
      "loss: 0.060675  [16016/60000]\n",
      "loss: 0.072888  [17616/60000]\n",
      "loss: 0.330664  [19216/60000]\n",
      "loss: 0.031506  [20816/60000]\n",
      "loss: 0.117535  [22416/60000]\n",
      "loss: 0.124103  [24016/60000]\n",
      "loss: 0.322017  [25616/60000]\n",
      "loss: 0.144923  [27216/60000]\n",
      "loss: 0.163135  [28816/60000]\n",
      "loss: 0.059521  [30416/60000]\n",
      "loss: 0.044405  [32016/60000]\n",
      "loss: 0.166059  [33616/60000]\n",
      "loss: 0.080696  [35216/60000]\n",
      "loss: 0.278742  [36816/60000]\n",
      "loss: 0.132211  [38416/60000]\n",
      "loss: 0.291242  [40016/60000]\n",
      "loss: 0.536454  [41616/60000]\n",
      "loss: 0.172102  [43216/60000]\n",
      "loss: 0.042634  [44816/60000]\n",
      "loss: 0.447969  [46416/60000]\n",
      "loss: 0.197300  [48016/60000]\n",
      "loss: 0.020697  [49616/60000]\n",
      "loss: 0.013808  [51216/60000]\n",
      "loss: 0.791030  [52816/60000]\n",
      "loss: 0.083692  [54416/60000]\n",
      "loss: 0.126586  [56016/60000]\n",
      "loss: 0.220132  [57616/60000]\n",
      "loss: 0.009644  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.4%, Avg loss: 0.082822 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.021851  [   16/60000]\n",
      "loss: 0.511843  [ 1616/60000]\n",
      "loss: 0.031374  [ 3216/60000]\n",
      "loss: 0.222188  [ 4816/60000]\n",
      "loss: 0.016462  [ 6416/60000]\n",
      "loss: 0.314949  [ 8016/60000]\n",
      "loss: 0.305112  [ 9616/60000]\n",
      "loss: 0.334211  [11216/60000]\n",
      "loss: 0.029163  [12816/60000]\n",
      "loss: 0.056853  [14416/60000]\n",
      "loss: 0.035043  [16016/60000]\n",
      "loss: 0.038464  [17616/60000]\n",
      "loss: 0.036989  [19216/60000]\n",
      "loss: 0.011851  [20816/60000]\n",
      "loss: 0.055767  [22416/60000]\n",
      "loss: 0.183480  [24016/60000]\n",
      "loss: 0.035898  [25616/60000]\n",
      "loss: 0.093196  [27216/60000]\n",
      "loss: 0.264254  [28816/60000]\n",
      "loss: 0.079951  [30416/60000]\n",
      "loss: 0.031040  [32016/60000]\n",
      "loss: 0.093184  [33616/60000]\n",
      "loss: 0.049135  [35216/60000]\n",
      "loss: 0.133943  [36816/60000]\n",
      "loss: 0.142680  [38416/60000]\n",
      "loss: 0.200369  [40016/60000]\n",
      "loss: 0.406848  [41616/60000]\n",
      "loss: 0.079485  [43216/60000]\n",
      "loss: 0.057344  [44816/60000]\n",
      "loss: 0.313141  [46416/60000]\n",
      "loss: 0.183415  [48016/60000]\n",
      "loss: 0.008089  [49616/60000]\n",
      "loss: 0.018764  [51216/60000]\n",
      "loss: 0.602549  [52816/60000]\n",
      "loss: 0.042816  [54416/60000]\n",
      "loss: 0.205472  [56016/60000]\n",
      "loss: 0.019936  [57616/60000]\n",
      "loss: 0.005726  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.071873 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.027165  [   16/60000]\n",
      "loss: 0.489396  [ 1616/60000]\n",
      "loss: 0.110078  [ 3216/60000]\n",
      "loss: 0.157464  [ 4816/60000]\n",
      "loss: 0.007381  [ 6416/60000]\n",
      "loss: 0.224039  [ 8016/60000]\n",
      "loss: 0.127194  [ 9616/60000]\n",
      "loss: 0.160992  [11216/60000]\n",
      "loss: 0.050026  [12816/60000]\n",
      "loss: 0.045700  [14416/60000]\n",
      "loss: 0.029685  [16016/60000]\n",
      "loss: 0.059693  [17616/60000]\n",
      "loss: 0.035100  [19216/60000]\n",
      "loss: 0.006584  [20816/60000]\n",
      "loss: 0.114359  [22416/60000]\n",
      "loss: 0.086069  [24016/60000]\n",
      "loss: 0.147050  [25616/60000]\n",
      "loss: 0.059816  [27216/60000]\n",
      "loss: 0.163956  [28816/60000]\n",
      "loss: 0.027581  [30416/60000]\n",
      "loss: 0.032075  [32016/60000]\n",
      "loss: 0.070350  [33616/60000]\n",
      "loss: 0.021621  [35216/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.063593  [36816/60000]\n",
      "loss: 0.150660  [38416/60000]\n",
      "loss: 0.144352  [40016/60000]\n",
      "loss: 0.316686  [41616/60000]\n",
      "loss: 0.333443  [43216/60000]\n",
      "loss: 0.045308  [44816/60000]\n",
      "loss: 0.295251  [46416/60000]\n",
      "loss: 0.184936  [48016/60000]\n",
      "loss: 0.031314  [49616/60000]\n",
      "loss: 0.013797  [51216/60000]\n",
      "loss: 0.582845  [52816/60000]\n",
      "loss: 0.057580  [54416/60000]\n",
      "loss: 0.089077  [56016/60000]\n",
      "loss: 0.140554  [57616/60000]\n",
      "loss: 0.028240  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.067841 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.005960  [   16/60000]\n",
      "loss: 0.756416  [ 1616/60000]\n",
      "loss: 0.098410  [ 3216/60000]\n",
      "loss: 0.185567  [ 4816/60000]\n",
      "loss: 0.010705  [ 6416/60000]\n",
      "loss: 0.272646  [ 8016/60000]\n",
      "loss: 0.221629  [ 9616/60000]\n",
      "loss: 0.379886  [11216/60000]\n",
      "loss: 0.013049  [12816/60000]\n",
      "loss: 0.065217  [14416/60000]\n",
      "loss: 0.039957  [16016/60000]\n",
      "loss: 0.024312  [17616/60000]\n",
      "loss: 0.072066  [19216/60000]\n",
      "loss: 0.006516  [20816/60000]\n",
      "loss: 0.183037  [22416/60000]\n",
      "loss: 0.221128  [24016/60000]\n",
      "loss: 0.105819  [25616/60000]\n",
      "loss: 0.224704  [27216/60000]\n",
      "loss: 0.227031  [28816/60000]\n",
      "loss: 0.047624  [30416/60000]\n",
      "loss: 0.022942  [32016/60000]\n",
      "loss: 0.063756  [33616/60000]\n",
      "loss: 0.071050  [35216/60000]\n",
      "loss: 0.049921  [36816/60000]\n",
      "loss: 0.033873  [38416/60000]\n",
      "loss: 0.180193  [40016/60000]\n",
      "loss: 0.275637  [41616/60000]\n",
      "loss: 0.053076  [43216/60000]\n",
      "loss: 0.139459  [44816/60000]\n",
      "loss: 0.426574  [46416/60000]\n",
      "loss: 0.185655  [48016/60000]\n",
      "loss: 0.043724  [49616/60000]\n",
      "loss: 0.013101  [51216/60000]\n",
      "loss: 0.376643  [52816/60000]\n",
      "loss: 0.049725  [54416/60000]\n",
      "loss: 0.069332  [56016/60000]\n",
      "loss: 0.006636  [57616/60000]\n",
      "loss: 0.012372  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.064973 \n",
      "\n",
      "Done!\n",
      "barch: 32\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.251092  [   32/60000]\n",
      "loss: 2.183653  [ 3232/60000]\n",
      "loss: 2.175655  [ 6432/60000]\n",
      "loss: 2.108335  [ 9632/60000]\n",
      "loss: 2.072353  [12832/60000]\n",
      "loss: 2.107218  [16032/60000]\n",
      "loss: 2.009366  [19232/60000]\n",
      "loss: 1.823354  [22432/60000]\n",
      "loss: 1.768074  [25632/60000]\n",
      "loss: 1.615896  [28832/60000]\n",
      "loss: 1.764290  [32032/60000]\n",
      "loss: 1.715309  [35232/60000]\n",
      "loss: 1.437109  [38432/60000]\n",
      "loss: 1.558165  [41632/60000]\n",
      "loss: 1.256104  [44832/60000]\n",
      "loss: 1.416523  [48032/60000]\n",
      "loss: 1.287223  [51232/60000]\n",
      "loss: 0.895883  [54432/60000]\n",
      "loss: 0.886698  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.772559 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.957956  [   32/60000]\n",
      "loss: 0.931973  [ 3232/60000]\n",
      "loss: 0.960268  [ 6432/60000]\n",
      "loss: 0.780746  [ 9632/60000]\n",
      "loss: 0.582059  [12832/60000]\n",
      "loss: 1.028614  [16032/60000]\n",
      "loss: 0.725541  [19232/60000]\n",
      "loss: 0.571018  [22432/60000]\n",
      "loss: 0.573059  [25632/60000]\n",
      "loss: 0.524319  [28832/60000]\n",
      "loss: 0.573960  [32032/60000]\n",
      "loss: 0.542107  [35232/60000]\n",
      "loss: 0.492483  [38432/60000]\n",
      "loss: 0.672189  [41632/60000]\n",
      "loss: 0.443373  [44832/60000]\n",
      "loss: 0.570561  [48032/60000]\n",
      "loss: 0.498698  [51232/60000]\n",
      "loss: 0.351628  [54432/60000]\n",
      "loss: 0.512884  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.1%, Avg loss: 0.338581 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.699149  [   32/60000]\n",
      "loss: 0.567361  [ 3232/60000]\n",
      "loss: 0.432988  [ 6432/60000]\n",
      "loss: 0.469440  [ 9632/60000]\n",
      "loss: 0.294903  [12832/60000]\n",
      "loss: 0.739369  [16032/60000]\n",
      "loss: 0.424710  [19232/60000]\n",
      "loss: 0.369477  [22432/60000]\n",
      "loss: 0.511488  [25632/60000]\n",
      "loss: 0.283991  [28832/60000]\n",
      "loss: 0.374281  [32032/60000]\n",
      "loss: 0.302585  [35232/60000]\n",
      "loss: 0.444602  [38432/60000]\n",
      "loss: 0.486434  [41632/60000]\n",
      "loss: 0.328083  [44832/60000]\n",
      "loss: 0.402516  [48032/60000]\n",
      "loss: 0.358665  [51232/60000]\n",
      "loss: 0.212981  [54432/60000]\n",
      "loss: 0.310603  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.208952 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.419519  [   32/60000]\n",
      "loss: 0.383037  [ 3232/60000]\n",
      "loss: 0.246533  [ 6432/60000]\n",
      "loss: 0.435765  [ 9632/60000]\n",
      "loss: 0.182012  [12832/60000]\n",
      "loss: 0.499382  [16032/60000]\n",
      "loss: 0.336679  [19232/60000]\n",
      "loss: 0.198095  [22432/60000]\n",
      "loss: 0.277457  [25632/60000]\n",
      "loss: 0.273676  [28832/60000]\n",
      "loss: 0.293290  [32032/60000]\n",
      "loss: 0.304109  [35232/60000]\n",
      "loss: 0.203359  [38432/60000]\n",
      "loss: 0.300975  [41632/60000]\n",
      "loss: 0.125845  [44832/60000]\n",
      "loss: 0.363474  [48032/60000]\n",
      "loss: 0.252016  [51232/60000]\n",
      "loss: 0.104833  [54432/60000]\n",
      "loss: 0.353672  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.2%, Avg loss: 0.168601 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.249226  [   32/60000]\n",
      "loss: 0.361237  [ 3232/60000]\n",
      "loss: 0.210661  [ 6432/60000]\n",
      "loss: 0.127981  [ 9632/60000]\n",
      "loss: 0.102921  [12832/60000]\n",
      "loss: 0.330108  [16032/60000]\n",
      "loss: 0.259191  [19232/60000]\n",
      "loss: 0.095286  [22432/60000]\n",
      "loss: 0.227717  [25632/60000]\n",
      "loss: 0.251899  [28832/60000]\n",
      "loss: 0.182925  [32032/60000]\n",
      "loss: 0.194832  [35232/60000]\n",
      "loss: 0.233404  [38432/60000]\n",
      "loss: 0.397930  [41632/60000]\n",
      "loss: 0.113907  [44832/60000]\n",
      "loss: 0.287244  [48032/60000]\n",
      "loss: 0.320420  [51232/60000]\n",
      "loss: 0.107500  [54432/60000]\n",
      "loss: 0.257201  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.145481 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.269448  [   32/60000]\n",
      "loss: 0.336845  [ 3232/60000]\n",
      "loss: 0.073090  [ 6432/60000]\n",
      "loss: 0.260268  [ 9632/60000]\n",
      "loss: 0.170495  [12832/60000]\n",
      "loss: 0.364296  [16032/60000]\n",
      "loss: 0.266343  [19232/60000]\n",
      "loss: 0.080732  [22432/60000]\n",
      "loss: 0.197294  [25632/60000]\n",
      "loss: 0.145757  [28832/60000]\n",
      "loss: 0.194161  [32032/60000]\n",
      "loss: 0.116120  [35232/60000]\n",
      "loss: 0.206500  [38432/60000]\n",
      "loss: 0.264849  [41632/60000]\n",
      "loss: 0.123582  [44832/60000]\n",
      "loss: 0.264011  [48032/60000]\n",
      "loss: 0.184513  [51232/60000]\n",
      "loss: 0.089818  [54432/60000]\n",
      "loss: 0.224404  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.123643 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.336505  [   32/60000]\n",
      "loss: 0.311307  [ 3232/60000]\n",
      "loss: 0.137337  [ 6432/60000]\n",
      "loss: 0.265864  [ 9632/60000]\n",
      "loss: 0.102734  [12832/60000]\n",
      "loss: 0.175663  [16032/60000]\n",
      "loss: 0.134662  [19232/60000]\n",
      "loss: 0.070447  [22432/60000]\n",
      "loss: 0.219579  [25632/60000]\n",
      "loss: 0.122988  [28832/60000]\n",
      "loss: 0.112966  [32032/60000]\n",
      "loss: 0.120706  [35232/60000]\n",
      "loss: 0.153813  [38432/60000]\n",
      "loss: 0.331249  [41632/60000]\n",
      "loss: 0.131061  [44832/60000]\n",
      "loss: 0.201565  [48032/60000]\n",
      "loss: 0.248274  [51232/60000]\n",
      "loss: 0.233774  [54432/60000]\n",
      "loss: 0.270854  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.7%, Avg loss: 0.107557 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.156058  [   32/60000]\n",
      "loss: 0.187168  [ 3232/60000]\n",
      "loss: 0.078480  [ 6432/60000]\n",
      "loss: 0.124675  [ 9632/60000]\n",
      "loss: 0.114696  [12832/60000]\n",
      "loss: 0.255489  [16032/60000]\n",
      "loss: 0.116037  [19232/60000]\n",
      "loss: 0.042339  [22432/60000]\n",
      "loss: 0.088514  [25632/60000]\n",
      "loss: 0.219673  [28832/60000]\n",
      "loss: 0.140263  [32032/60000]\n",
      "loss: 0.158060  [35232/60000]\n",
      "loss: 0.155997  [38432/60000]\n",
      "loss: 0.354153  [41632/60000]\n",
      "loss: 0.082003  [44832/60000]\n",
      "loss: 0.110598  [48032/60000]\n",
      "loss: 0.171752  [51232/60000]\n",
      "loss: 0.080664  [54432/60000]\n",
      "loss: 0.212231  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.102865 \n",
      "\n",
      "Done!\n",
      "barch: 64\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.430374  [   64/60000]\n",
      "loss: 2.265802  [ 6464/60000]\n",
      "loss: 2.264072  [12864/60000]\n",
      "loss: 2.144408  [19264/60000]\n",
      "loss: 2.209450  [25664/60000]\n",
      "loss: 2.173131  [32064/60000]\n",
      "loss: 2.005733  [38464/60000]\n",
      "loss: 2.042843  [44864/60000]\n",
      "loss: 2.012072  [51264/60000]\n",
      "loss: 1.852802  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.5%, Avg loss: 1.824595 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.836637  [   64/60000]\n",
      "loss: 1.756019  [ 6464/60000]\n",
      "loss: 1.800471  [12864/60000]\n",
      "loss: 1.728493  [19264/60000]\n",
      "loss: 1.579746  [25664/60000]\n",
      "loss: 1.572840  [32064/60000]\n",
      "loss: 1.356952  [38464/60000]\n",
      "loss: 1.551219  [44864/60000]\n",
      "loss: 1.466289  [51264/60000]\n",
      "loss: 1.247925  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.5%, Avg loss: 1.101327 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.194537  [   64/60000]\n",
      "loss: 1.216342  [ 6464/60000]\n",
      "loss: 1.057722  [12864/60000]\n",
      "loss: 1.059637  [19264/60000]\n",
      "loss: 1.012574  [25664/60000]\n",
      "loss: 1.024218  [32064/60000]\n",
      "loss: 0.785426  [38464/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.980879  [44864/60000]\n",
      "loss: 0.904926  [51264/60000]\n",
      "loss: 0.840129  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.8%, Avg loss: 0.657755 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.854990  [   64/60000]\n",
      "loss: 0.686017  [ 6464/60000]\n",
      "loss: 0.643786  [12864/60000]\n",
      "loss: 0.759437  [19264/60000]\n",
      "loss: 0.812538  [25664/60000]\n",
      "loss: 0.736196  [32064/60000]\n",
      "loss: 0.595644  [38464/60000]\n",
      "loss: 0.675804  [44864/60000]\n",
      "loss: 0.770847  [51264/60000]\n",
      "loss: 0.657515  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.436784 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.567633  [   64/60000]\n",
      "loss: 0.438911  [ 6464/60000]\n",
      "loss: 0.550717  [12864/60000]\n",
      "loss: 0.470087  [19264/60000]\n",
      "loss: 0.506407  [25664/60000]\n",
      "loss: 0.523330  [32064/60000]\n",
      "loss: 0.361662  [38464/60000]\n",
      "loss: 0.515291  [44864/60000]\n",
      "loss: 0.556012  [51264/60000]\n",
      "loss: 0.518698  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.315416 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.453774  [   64/60000]\n",
      "loss: 0.420917  [ 6464/60000]\n",
      "loss: 0.373370  [12864/60000]\n",
      "loss: 0.304193  [19264/60000]\n",
      "loss: 0.431363  [25664/60000]\n",
      "loss: 0.426488  [32064/60000]\n",
      "loss: 0.319027  [38464/60000]\n",
      "loss: 0.399366  [44864/60000]\n",
      "loss: 0.401521  [51264/60000]\n",
      "loss: 0.318415  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.244999 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.334215  [   64/60000]\n",
      "loss: 0.389522  [ 6464/60000]\n",
      "loss: 0.357485  [12864/60000]\n",
      "loss: 0.293727  [19264/60000]\n",
      "loss: 0.297010  [25664/60000]\n",
      "loss: 0.440172  [32064/60000]\n",
      "loss: 0.244528  [38464/60000]\n",
      "loss: 0.400483  [44864/60000]\n",
      "loss: 0.351418  [51264/60000]\n",
      "loss: 0.412478  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.201200 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.300147  [   64/60000]\n",
      "loss: 0.271834  [ 6464/60000]\n",
      "loss: 0.276704  [12864/60000]\n",
      "loss: 0.303319  [19264/60000]\n",
      "loss: 0.267940  [25664/60000]\n",
      "loss: 0.451304  [32064/60000]\n",
      "loss: 0.267845  [38464/60000]\n",
      "loss: 0.351841  [44864/60000]\n",
      "loss: 0.388827  [51264/60000]\n",
      "loss: 0.306304  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.171327 \n",
      "\n",
      "Done!\n",
      "barch: 128\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.353724  [  128/60000]\n",
      "loss: 2.287821  [12928/60000]\n",
      "loss: 2.262918  [25728/60000]\n",
      "loss: 2.196946  [38528/60000]\n",
      "loss: 2.207638  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 53.4%, Avg loss: 2.113469 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.106014  [  128/60000]\n",
      "loss: 2.102657  [12928/60000]\n",
      "loss: 2.023607  [25728/60000]\n",
      "loss: 1.989681  [38528/60000]\n",
      "loss: 1.997646  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.8%, Avg loss: 1.870793 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.833466  [  128/60000]\n",
      "loss: 1.893082  [12928/60000]\n",
      "loss: 1.771308  [25728/60000]\n",
      "loss: 1.656607  [38528/60000]\n",
      "loss: 1.728256  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 1.490621 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.481360  [  128/60000]\n",
      "loss: 1.501943  [12928/60000]\n",
      "loss: 1.332785  [25728/60000]\n",
      "loss: 1.258860  [38528/60000]\n",
      "loss: 1.331577  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 1.038643 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.144975  [  128/60000]\n",
      "loss: 1.098286  [12928/60000]\n",
      "loss: 0.941326  [25728/60000]\n",
      "loss: 0.915295  [38528/60000]\n",
      "loss: 1.019416  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.718985 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.808255  [  128/60000]\n",
      "loss: 0.745610  [12928/60000]\n",
      "loss: 0.666870  [25728/60000]\n",
      "loss: 0.690716  [38528/60000]\n",
      "loss: 0.781177  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.7%, Avg loss: 0.526209 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.599367  [  128/60000]\n",
      "loss: 0.538383  [12928/60000]\n",
      "loss: 0.567328  [25728/60000]\n",
      "loss: 0.687834  [38528/60000]\n",
      "loss: 0.689707  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.414705 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.468695  [  128/60000]\n",
      "loss: 0.481974  [12928/60000]\n",
      "loss: 0.514911  [25728/60000]\n",
      "loss: 0.503456  [38528/60000]\n",
      "loss: 0.609404  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.7%, Avg loss: 0.341850 \n",
      "\n",
      "Done!\n",
      "barch: 256\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.348064  [  256/60000]\n",
      "loss: 2.268492  [25856/60000]\n",
      "loss: 2.191918  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.9%, Avg loss: 2.131814 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.179393  [  256/60000]\n",
      "loss: 2.112024  [25856/60000]\n",
      "loss: 2.003944  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 1.969120 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.010612  [  256/60000]\n",
      "loss: 1.909515  [25856/60000]\n",
      "loss: 1.858609  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 1.746153 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.776144  [  256/60000]\n",
      "loss: 1.668372  [25856/60000]\n",
      "loss: 1.641578  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 1.472488 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.499869  [  256/60000]\n",
      "loss: 1.399856  [25856/60000]\n",
      "loss: 1.379729  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.9%, Avg loss: 1.206770 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.267693  [  256/60000]\n",
      "loss: 1.211437  [25856/60000]\n",
      "loss: 1.180867  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.992934 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.059721  [  256/60000]\n",
      "loss: 1.014647  [25856/60000]\n",
      "loss: 0.987766  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.831562 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.949778  [  256/60000]\n",
      "loss: 0.890945  [25856/60000]\n",
      "loss: 0.869743  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.4%, Avg loss: 0.705465 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同batchsize的影响\n",
    "batchSizeList = [16, 32, 64, 128, 256]\n",
    "batch_loss = []\n",
    "\n",
    "for batch in batchSizeList:\n",
    "    model = CNN_1().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch)\n",
    "    print(f\"barch: {batch}\\n-------------------------------------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    batch_loss.append(test_loss)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ea212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: MSELoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.357478  [   64/60000]\n",
      "loss: 0.162833  [ 6464/60000]\n",
      "loss: 0.143348  [12864/60000]\n",
      "loss: 0.134861  [19264/60000]\n",
      "loss: 0.134233  [25664/60000]\n",
      "loss: 0.113897  [32064/60000]\n",
      "loss: 0.117590  [38464/60000]\n",
      "loss: 0.120766  [44864/60000]\n",
      "loss: 0.115254  [51264/60000]\n",
      "loss: 0.105012  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 0.084491 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.103885  [   64/60000]\n",
      "loss: 0.106563  [ 6464/60000]\n",
      "loss: 0.103956  [12864/60000]\n",
      "loss: 0.097110  [19264/60000]\n",
      "loss: 0.095295  [25664/60000]\n",
      "loss: 0.098284  [32064/60000]\n",
      "loss: 0.092856  [38464/60000]\n",
      "loss: 0.101634  [44864/60000]\n",
      "loss: 0.099118  [51264/60000]\n",
      "loss: 0.091363  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 41.9%, Avg loss: 0.080841 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.091828  [   64/60000]\n",
      "loss: 0.093250  [ 6464/60000]\n",
      "loss: 0.092857  [12864/60000]\n",
      "loss: 0.090673  [19264/60000]\n",
      "loss: 0.094402  [25664/60000]\n",
      "loss: 0.090815  [32064/60000]\n",
      "loss: 0.086969  [38464/60000]\n",
      "loss: 0.083574  [44864/60000]\n",
      "loss: 0.093071  [51264/60000]\n",
      "loss: 0.085356  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Avg loss: 0.078442 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.090129  [   64/60000]\n",
      "loss: 0.086286  [ 6464/60000]\n",
      "loss: 0.084631  [12864/60000]\n",
      "loss: 0.088755  [19264/60000]\n",
      "loss: 0.082973  [25664/60000]\n",
      "loss: 0.084470  [32064/60000]\n",
      "loss: 0.082209  [38464/60000]\n",
      "loss: 0.086989  [44864/60000]\n",
      "loss: 0.085599  [51264/60000]\n",
      "loss: 0.084133  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 54.7%, Avg loss: 0.076304 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.085956  [   64/60000]\n",
      "loss: 0.079077  [ 6464/60000]\n",
      "loss: 0.079940  [12864/60000]\n",
      "loss: 0.082751  [19264/60000]\n",
      "loss: 0.079849  [25664/60000]\n",
      "loss: 0.083626  [32064/60000]\n",
      "loss: 0.078689  [38464/60000]\n",
      "loss: 0.082347  [44864/60000]\n",
      "loss: 0.084729  [51264/60000]\n"
     ]
    }
   ],
   "source": [
    "# 不同损失函数的影响\n",
    "loss_fnList = [nn.MSELoss(), nn.CrossEntropyLoss()]\n",
    "fn_loss = []\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for loss_fn in loss_fnList:\n",
    "    model = CNN_1().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "    flag = 0\n",
    "    if isinstance(loss_fn, nn.MSELoss):\n",
    "        flag = 1\n",
    "    else:\n",
    "        flag = 0\n",
    "    print(f\"loss: {loss_fn}\\n-------------------------------------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, flag = flag)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    fn_loss.append(test_loss)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2aa0e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: CrossEntropyLoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.285084  [   64/60000]\n",
      "loss: 2.209720  [ 6464/60000]\n",
      "loss: 2.166452  [12864/60000]\n",
      "loss: 2.191889  [19264/60000]\n",
      "loss: 2.068367  [25664/60000]\n",
      "loss: 2.051772  [32064/60000]\n",
      "loss: 1.814354  [38464/60000]\n",
      "loss: 1.884966  [44864/60000]\n",
      "loss: 1.856793  [51264/60000]\n",
      "loss: 1.662924  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 1.646492 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.648886  [   64/60000]\n",
      "loss: 1.570307  [ 6464/60000]\n",
      "loss: 1.553126  [12864/60000]\n",
      "loss: 1.491133  [19264/60000]\n",
      "loss: 1.456645  [25664/60000]\n",
      "loss: 1.382655  [32064/60000]\n",
      "loss: 1.136319  [38464/60000]\n",
      "loss: 1.194892  [44864/60000]\n",
      "loss: 1.225872  [51264/60000]\n",
      "loss: 0.987676  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.5%, Avg loss: 0.961822 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.006480  [   64/60000]\n",
      "loss: 1.124630  [ 6464/60000]\n",
      "loss: 1.005863  [12864/60000]\n",
      "loss: 0.969076  [19264/60000]\n",
      "loss: 0.920413  [25664/60000]\n",
      "loss: 0.839205  [32064/60000]\n",
      "loss: 0.794756  [38464/60000]\n",
      "loss: 0.707636  [44864/60000]\n",
      "loss: 0.862231  [51264/60000]\n",
      "loss: 0.733064  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.4%, Avg loss: 0.609330 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.640105  [   64/60000]\n",
      "loss: 0.741205  [ 6464/60000]\n",
      "loss: 0.659144  [12864/60000]\n",
      "loss: 0.762609  [19264/60000]\n",
      "loss: 0.627551  [25664/60000]\n",
      "loss: 0.762043  [32064/60000]\n",
      "loss: 0.505217  [38464/60000]\n",
      "loss: 0.563057  [44864/60000]\n",
      "loss: 0.726763  [51264/60000]\n",
      "loss: 0.664756  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.5%, Avg loss: 0.421090 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.544967  [   64/60000]\n",
      "loss: 0.479465  [ 6464/60000]\n",
      "loss: 0.479326  [12864/60000]\n",
      "loss: 0.489614  [19264/60000]\n",
      "loss: 0.517100  [25664/60000]\n",
      "loss: 0.517319  [32064/60000]\n",
      "loss: 0.404501  [38464/60000]\n",
      "loss: 0.447725  [44864/60000]\n",
      "loss: 0.620644  [51264/60000]\n",
      "loss: 0.459507  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.2%, Avg loss: 0.312494 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.416841  [   64/60000]\n",
      "loss: 0.381303  [ 6464/60000]\n",
      "loss: 0.333330  [12864/60000]\n",
      "loss: 0.499100  [19264/60000]\n",
      "loss: 0.360143  [25664/60000]\n",
      "loss: 0.395220  [32064/60000]\n",
      "loss: 0.283891  [38464/60000]\n",
      "loss: 0.325891  [44864/60000]\n",
      "loss: 0.519328  [51264/60000]\n",
      "loss: 0.396760  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.254745 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.363602  [   64/60000]\n",
      "loss: 0.336694  [ 6464/60000]\n",
      "loss: 0.289014  [12864/60000]\n",
      "loss: 0.396963  [19264/60000]\n",
      "loss: 0.375150  [25664/60000]\n",
      "loss: 0.403226  [32064/60000]\n",
      "loss: 0.286558  [38464/60000]\n",
      "loss: 0.326009  [44864/60000]\n",
      "loss: 0.390656  [51264/60000]\n",
      "loss: 0.352837  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.5%, Avg loss: 0.210818 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.212152  [   64/60000]\n",
      "loss: 0.321933  [ 6464/60000]\n",
      "loss: 0.259166  [12864/60000]\n",
      "loss: 0.273564  [19264/60000]\n",
      "loss: 0.255808  [25664/60000]\n",
      "loss: 0.309716  [32064/60000]\n",
      "loss: 0.315232  [38464/60000]\n",
      "loss: 0.300240  [44864/60000]\n",
      "loss: 0.367412  [51264/60000]\n",
      "loss: 0.385323  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.175322 \n",
      "\n",
      "Done!\n",
      "loss: CrossEntropyLoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.436008  [   64/60000]\n",
      "loss: 0.273266  [ 6464/60000]\n",
      "loss: 0.188301  [12864/60000]\n",
      "loss: 0.238461  [19264/60000]\n",
      "loss: 0.153474  [25664/60000]\n",
      "loss: 0.075805  [32064/60000]\n",
      "loss: 0.078627  [38464/60000]\n",
      "loss: 0.168788  [44864/60000]\n",
      "loss: 0.217519  [51264/60000]\n",
      "loss: 0.225504  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.0%, Avg loss: 0.084044 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.088784  [   64/60000]\n",
      "loss: 0.132982  [ 6464/60000]\n",
      "loss: 0.164638  [12864/60000]\n",
      "loss: 0.112683  [19264/60000]\n",
      "loss: 0.054461  [25664/60000]\n",
      "loss: 0.034905  [32064/60000]\n",
      "loss: 0.043451  [38464/60000]\n",
      "loss: 0.050489  [44864/60000]\n",
      "loss: 0.183359  [51264/60000]\n",
      "loss: 0.096602  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.2%, Avg loss: 0.048953 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.088288  [   64/60000]\n",
      "loss: 0.133169  [ 6464/60000]\n",
      "loss: 0.125164  [12864/60000]\n",
      "loss: 0.035526  [19264/60000]\n",
      "loss: 0.017892  [25664/60000]\n",
      "loss: 0.039658  [32064/60000]\n",
      "loss: 0.043634  [38464/60000]\n",
      "loss: 0.048599  [44864/60000]\n",
      "loss: 0.125942  [51264/60000]\n",
      "loss: 0.055208  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.046777 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.023321  [   64/60000]\n",
      "loss: 0.105147  [ 6464/60000]\n",
      "loss: 0.098168  [12864/60000]\n",
      "loss: 0.102101  [19264/60000]\n",
      "loss: 0.014686  [25664/60000]\n",
      "loss: 0.027323  [32064/60000]\n",
      "loss: 0.031590  [38464/60000]\n",
      "loss: 0.030663  [44864/60000]\n",
      "loss: 0.088300  [51264/60000]\n",
      "loss: 0.153724  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.038895 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.037504  [   64/60000]\n",
      "loss: 0.147361  [ 6464/60000]\n",
      "loss: 0.077240  [12864/60000]\n",
      "loss: 0.028948  [19264/60000]\n",
      "loss: 0.065070  [25664/60000]\n",
      "loss: 0.018433  [32064/60000]\n",
      "loss: 0.064799  [38464/60000]\n",
      "loss: 0.058916  [44864/60000]\n",
      "loss: 0.111683  [51264/60000]\n",
      "loss: 0.074026  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.034640 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.010122  [   64/60000]\n",
      "loss: 0.016951  [ 6464/60000]\n",
      "loss: 0.040426  [12864/60000]\n",
      "loss: 0.118467  [19264/60000]\n",
      "loss: 0.010322  [25664/60000]\n",
      "loss: 0.023421  [32064/60000]\n",
      "loss: 0.012317  [38464/60000]\n",
      "loss: 0.025181  [44864/60000]\n",
      "loss: 0.148037  [51264/60000]\n",
      "loss: 0.126952  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.055529 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.036538  [   64/60000]\n",
      "loss: 0.120240  [ 6464/60000]\n",
      "loss: 0.059678  [12864/60000]\n",
      "loss: 0.032878  [19264/60000]\n",
      "loss: 0.056173  [25664/60000]\n",
      "loss: 0.027087  [32064/60000]\n",
      "loss: 0.073162  [38464/60000]\n",
      "loss: 0.091788  [44864/60000]\n",
      "loss: 0.064392  [51264/60000]\n",
      "loss: 0.078544  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.039136 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.024474  [   64/60000]\n",
      "loss: 0.024757  [ 6464/60000]\n",
      "loss: 0.050380  [12864/60000]\n",
      "loss: 0.019851  [19264/60000]\n",
      "loss: 0.032455  [25664/60000]\n",
      "loss: 0.031631  [32064/60000]\n",
      "loss: 0.060852  [38464/60000]\n",
      "loss: 0.026882  [44864/60000]\n",
      "loss: 0.042387  [51264/60000]\n",
      "loss: 0.102961  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.035390 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同优化器的影响\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "optim_loss = []\n",
    "for i in range(2):\n",
    "    model = CNN_1().to(device)\n",
    "    # 进行优化器选择\n",
    "    if i == 0:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "    if i == 1:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "    print(f\"loss: {loss_fn}\\n-------------------------------------------------------------\")\n",
    "    # 进行训练\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    optim_loss.append(test_loss)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c72d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
