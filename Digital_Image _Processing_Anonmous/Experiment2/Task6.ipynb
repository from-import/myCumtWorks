{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9fe8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入相关库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7888711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数定义\n",
    "#dataset = \"FashionMNIST\" #选择所使用的数据集\"MNIST\"or\"FashionMNIST\"\n",
    "dataset = \"MNIST\"\n",
    "Net = \"LeNet5\" #选择所使用的网络\"LeNet5\"or \"LinearNet\"\n",
    "# Net = \"LinearNet\" #选择所使用的网络\"LeNet5\"or \"LinearNet\"\n",
    "batch_size = 64 #定义批处理大小\n",
    "lr=0.01 #设置learning rate学习率\n",
    "epochs = 8 #指定训练迭代次数\n",
    "save_path = \"./\" #模型保存路径\n",
    "Early_Stopping = 0 #选择是否使用Early Stopping训练模式，训练时根据精度的变化率来控制训练迭代代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a01ee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#下载数据集\n",
    "if dataset==\"MNIST\":\n",
    "    # 从torchvision下载训练集.\n",
    "    training_data = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # #从torchvision下载测试集.\n",
    "    test_data = datasets.MNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    # 若是数字数据集则设置成下述类别\n",
    "    classes = [ \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "else:\n",
    "    # 从torchvision下载训练集.\n",
    "    training_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "\n",
    "    # #从torchvision下载测试集.\n",
    "    test_data = datasets.FashionMNIST(\n",
    "        root=\"data\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "    # 若是服饰数据集则设置成下述类别\n",
    "    classes = [ \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\",\"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905c5adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 创建数据加载器.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "#观察数据样本\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    fig = plt.figure()\n",
    "    # 对部分数据进行显示\n",
    "    for i in range(6):\n",
    "      plt.subplot(2,3,i+1)\n",
    "      plt.tight_layout()\n",
    "      plt.imshow(X[i][0], cmap='gray', interpolation='none')\n",
    "      plt.title(\"Ground Truth: {}\".format(classes[int(y[i])]))\n",
    "      plt.xticks([])\n",
    "      plt.yticks([])\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08974297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# 自动选择cpu或gpu用于训练.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c7b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    残差块定义：\n",
    "    参数：\n",
    "        in_channels：输入特征的通道数\n",
    "        out_channels: 输出特征的通道数\n",
    "        stride: 步长\n",
    "        downsample: 如果输入输出不匹配，用于下采样的模块\n",
    "'''\n",
    "\n",
    "class ResidualBlock(nn.Module):  \n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):  \n",
    "        super(ResidualBlock, self).__init__()  \n",
    "            \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) \n",
    "        \n",
    "        # 进行批量归一化\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU(inplace=True)  \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)  \n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)  \n",
    "          \n",
    "        # 短路连接（如果输入输出通道数或特征图大小不同，需要进行下采样）  \n",
    "        self.downsample = downsample  \n",
    "          \n",
    "    def forward(self, x):  \n",
    "        residual = x\n",
    "          \n",
    "        # 主路径\n",
    "#         print(x.shape)\n",
    "        out = self.conv1(x)  \n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)  \n",
    "        out = self.conv2(out)  \n",
    "        out = self.bn2(out)  \n",
    "          \n",
    "        # 短路连接  \n",
    "        if self.downsample is not None:  \n",
    "            residual = self.downsample(x)  \n",
    "          \n",
    "        # 残差连接  \n",
    "        # 将主路径和短路连接的结果相加\n",
    "        out += residual  \n",
    "        out = self.relu(out)  \n",
    "        \n",
    "        # 返回经过残差处理后的特征图\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274dd484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1(\n",
      "  (conv1): Conv2d(1, 13, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2d): Sequential(\n",
      "    (0): Conv2d(13, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (res_block): ResidualBlock(\n",
      "    (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(10, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out1): Linear(in_features=261, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#定义模型\n",
    "\n",
    "if Net == \"LinearNet\":\n",
    "    # 普通神经网络\n",
    "    class LinearNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_relu_stack = nn.Sequential(\n",
    "                nn.Linear(28*28, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, 10)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            logits = self.linear_relu_stack(x)\n",
    "            return logits\n",
    "\n",
    "    model = LinearNet().to(device)\n",
    "    \n",
    "else:\n",
    "    class CNN_1(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(CNN_1, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                    ## 卷积核大小为羽3，输入通道为__，输出通道为13，步长为1;\n",
    "                    ## paddingHzero padding\n",
    "                    ##要求经过conv1的输出空间维度与输入的空间维度相同\n",
    "                    in_channels = 1,\n",
    "                    out_channels = 13,\n",
    "                    kernel_size = 3,\n",
    "                    stride= 1,\n",
    "                    padding= 1,\n",
    "                )\n",
    "            ##激活函数+最大池化\n",
    "            self.relu = nn.ReLU()\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "            self.conv2d = nn.Sequential(\n",
    "            ## 1. 卷积核大小为3*3，输入通道为10，输出通道为10，padding 方法为same，padding大小为？？？步长为??\n",
    "            ## 2.自行选择激活函数\n",
    "            ## 3. 池化\n",
    "                nn.Conv2d(in_channels=13,\n",
    "                         out_channels=10,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1),\n",
    "                self.relu,\n",
    "                self.pool\n",
    "            )\n",
    "            ## 添加残差连接模块\n",
    "            #############残差模块设计部分################\n",
    "            self.res_block = ResidualBlock(10, 10)\n",
    "\n",
    "            ############################################\n",
    "            self.conv3 = nn.Sequential(\n",
    "                ##自行设计卷积模块\n",
    "                nn.Conv2d(in_channels=10,\n",
    "                         out_channels=29,\n",
    "                         kernel_size=3,\n",
    "                         stride=1,\n",
    "                         padding=1),\n",
    "                self.relu,\n",
    "                self.pool\n",
    "            )\n",
    "            self.out1 = nn.Linear(3*3*29, 10, bias = True)\n",
    "            ## 在下方添加Dropout以及其他代码\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        def forward(self, x):\n",
    "            ## 请将余下代码补充完整\n",
    "            # 卷积后图像大小保持不变\n",
    "            # (64,1,28,28)\n",
    "            x = self.relu(self.conv1(x))\n",
    "            x = self.pool(x)\n",
    "            # (64,13,14,14)\n",
    "            x = self.conv2d(x)\n",
    "            # (64,10,7,7)\n",
    "            x = self.res_block(x)\n",
    "            # (64,10,7,7)\n",
    "            x = self.conv3(x)\n",
    "            # (64,10,3,3)\n",
    "            \n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(x)\n",
    "            x = self.out1(x)\n",
    "            \n",
    "            return x\n",
    "            \n",
    "\n",
    "    model = CNN_1().to(device)\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c81349",
   "metadata": {},
   "outputs": [],
   "source": [
    "#为了训练模型，我们需要一个损失函数和一个优化器\n",
    "loss_fn = nn.CrossEntropyLoss()#定义损失函数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "\n",
    "#在单个训练循环中，模型对训练数据集进行预测（分批提供给它），并反向传播预测误差以调整模型的参数\n",
    "def train(dataloader, model, loss_fn, optimizer, flag=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        tensor_new = y\n",
    "        if flag == 1:\n",
    "            tensor_new = torch.zeros(y.shape[0], 10)\n",
    "            for i in range(len(y)):\n",
    "                tensor_new[i,y[i]] = 1\n",
    "        # Compute prediction error\n",
    "        tensor_new = tensor_new.to(device)\n",
    "        pred = model(X)\n",
    "#         print(pred.shape)\n",
    "#         print(tensor_new.shape)\n",
    "        loss = loss_fn(pred, tensor_new)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "#对照测试数据集检查模型的性能，以确保它正在学习\n",
    "def test(dataloader, model, loss_fn, flag=0):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            tensor_new = y\n",
    "            if flag == 1:\n",
    "                tensor_new = torch.zeros(y.shape[0], 10)\n",
    "                for i in range(len(y)):\n",
    "                    tensor_new[i,y[i]] = 1\n",
    "            tensor_new = tensor_new.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, tensor_new).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "    \n",
    "#实现Early Stopping训练\n",
    "class EarlyStopping:\n",
    "    def __init__(self, save_path, patience=2, verbose=False, delta=0.03):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            save_path : 模型保存路径\n",
    "            patience (int): 设置将连续几次训练迭代纳入Early Stopping考评\n",
    "            verbose (bool): 如果是 \"True\"，则为每次验证损失的优化值打印一条信息\n",
    "            delta (float): 前后两次训练迭代的最小变化阈值，小于该阈值则认为模型优化幅度有限，将该次迭代计入patience，\n",
    "                           数量达到patience则提前停止训练。\n",
    "        \"\"\"\n",
    "        self.save_path = save_path\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        # 尚未找到最佳的验证分数\n",
    "        if self.best_score is None:\n",
    "            # 将当前的分数作为最佳分数\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        # 如果分数没有得到明显改善\n",
    "        elif score < self.best_score + self.delta:\n",
    "            # 计时器加1操作\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # 如果超出一定时间，说明达到最优，则进行保存\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # 如果得到明显改善，说明还未达到最优，则进行存储，同时继续进行查找\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''当验证损失降低时，保存模型'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        path = os.path.join(self.save_path, 'best_network.pth')\n",
    "        torch.save(model.state_dict(), path)\t# 这里会存储迄今最优模型的参数\n",
    "        self.val_loss_min = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3024dd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.043084  [   64/60000]\n",
      "loss: 0.022173  [ 6464/60000]\n",
      "loss: 0.055733  [12864/60000]\n",
      "loss: 0.013116  [19264/60000]\n",
      "loss: 0.076859  [25664/60000]\n",
      "loss: 0.042566  [32064/60000]\n",
      "loss: 0.011415  [38464/60000]\n",
      "loss: 0.023089  [44864/60000]\n",
      "loss: 0.180605  [51264/60000]\n",
      "loss: 0.014221  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.037902 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.050694  [   64/60000]\n",
      "loss: 0.059616  [ 6464/60000]\n",
      "loss: 0.045975  [12864/60000]\n",
      "loss: 0.063846  [19264/60000]\n",
      "loss: 0.137655  [25664/60000]\n",
      "loss: 0.105314  [32064/60000]\n",
      "loss: 0.012601  [38464/60000]\n",
      "loss: 0.012116  [44864/60000]\n",
      "loss: 0.101711  [51264/60000]\n",
      "loss: 0.015794  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.048066 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.009560  [   64/60000]\n",
      "loss: 0.048139  [ 6464/60000]\n",
      "loss: 0.039090  [12864/60000]\n",
      "loss: 0.018426  [19264/60000]\n",
      "loss: 0.115241  [25664/60000]\n",
      "loss: 0.011705  [32064/60000]\n",
      "loss: 0.016725  [38464/60000]\n",
      "loss: 0.017746  [44864/60000]\n",
      "loss: 0.124420  [51264/60000]\n",
      "loss: 0.073007  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.1%, Avg loss: 0.030288 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.015475  [   64/60000]\n",
      "loss: 0.032580  [ 6464/60000]\n",
      "loss: 0.041461  [12864/60000]\n",
      "loss: 0.026588  [19264/60000]\n",
      "loss: 0.059852  [25664/60000]\n",
      "loss: 0.046415  [32064/60000]\n",
      "loss: 0.023501  [38464/60000]\n",
      "loss: 0.002681  [44864/60000]\n",
      "loss: 0.145338  [51264/60000]\n",
      "loss: 0.046149  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031356 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.037379  [   64/60000]\n",
      "loss: 0.013216  [ 6464/60000]\n",
      "loss: 0.010958  [12864/60000]\n",
      "loss: 0.019017  [19264/60000]\n",
      "loss: 0.099419  [25664/60000]\n",
      "loss: 0.051250  [32064/60000]\n",
      "loss: 0.005957  [38464/60000]\n",
      "loss: 0.003845  [44864/60000]\n",
      "loss: 0.151240  [51264/60000]\n",
      "loss: 0.016940  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.050015 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.048979  [   64/60000]\n",
      "loss: 0.017265  [ 6464/60000]\n",
      "loss: 0.018828  [12864/60000]\n",
      "loss: 0.016447  [19264/60000]\n",
      "loss: 0.017716  [25664/60000]\n",
      "loss: 0.043604  [32064/60000]\n",
      "loss: 0.010848  [38464/60000]\n",
      "loss: 0.006774  [44864/60000]\n",
      "loss: 0.161488  [51264/60000]\n",
      "loss: 0.026795  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.033343 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.016433  [   64/60000]\n",
      "loss: 0.005777  [ 6464/60000]\n",
      "loss: 0.135938  [12864/60000]\n",
      "loss: 0.025811  [19264/60000]\n",
      "loss: 0.010837  [25664/60000]\n",
      "loss: 0.011034  [32064/60000]\n",
      "loss: 0.005310  [38464/60000]\n",
      "loss: 0.016896  [44864/60000]\n",
      "loss: 0.147127  [51264/60000]\n",
      "loss: 0.027719  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.044897 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.013571  [   64/60000]\n",
      "loss: 0.019835  [ 6464/60000]\n",
      "loss: 0.008380  [12864/60000]\n",
      "loss: 0.007864  [19264/60000]\n",
      "loss: 0.034115  [25664/60000]\n",
      "loss: 0.035810  [32064/60000]\n",
      "loss: 0.016193  [38464/60000]\n",
      "loss: 0.009277  [44864/60000]\n",
      "loss: 0.160352  [51264/60000]\n",
      "loss: 0.004307  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031917 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#训练过程是在所定义的几个迭代上进行的。在每次迭代，模型学习参数以做出更好的预测。我们在每次迭代打印模型的准确性和损失；我们希望看到精度随着迭代次数的增加而增加，而损失随着迭代次数的减少而减少\n",
    "early_stopping = EarlyStopping(save_path)\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loss = test(test_dataloader, model, loss_fn)\n",
    "    if Early_Stopping:\n",
    "        early_stopping(test_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break #跳出迭代，结束训练\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "769df87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "#保存模型的一种常见方法是序列化内部状态字典(包含模型参数)\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ef49333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"7\", Actual: \"7\"\n"
     ]
    }
   ],
   "source": [
    "#加载模型的过程包括重新创建模型结构并将状态字典加载到其中。\n",
    "model = LinearNet().to(device) if Net == \"LinearNet\" else CNN_1().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "with torch.no_grad():\n",
    "    x = x.unsqueeze_(0).to(device)\n",
    "#     x = x.to(device)\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f885759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr: 0.001\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.306097  [   64/60000]\n",
      "loss: 2.247044  [ 6464/60000]\n",
      "loss: 2.303789  [12864/60000]\n",
      "loss: 2.194507  [19264/60000]\n",
      "loss: 2.133592  [25664/60000]\n",
      "loss: 2.135555  [32064/60000]\n",
      "loss: 1.918354  [38464/60000]\n",
      "loss: 2.004908  [44864/60000]\n",
      "loss: 1.921607  [51264/60000]\n",
      "loss: 1.834036  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.9%, Avg loss: 1.675255 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.750150  [   64/60000]\n",
      "loss: 1.572982  [ 6464/60000]\n",
      "loss: 1.565745  [12864/60000]\n",
      "loss: 1.518401  [19264/60000]\n",
      "loss: 1.502563  [25664/60000]\n",
      "loss: 1.258546  [32064/60000]\n",
      "loss: 1.134379  [38464/60000]\n",
      "loss: 1.146959  [44864/60000]\n",
      "loss: 1.194839  [51264/60000]\n",
      "loss: 0.989342  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.807195 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.966556  [   64/60000]\n",
      "loss: 0.912614  [ 6464/60000]\n",
      "loss: 0.858950  [12864/60000]\n",
      "loss: 0.802690  [19264/60000]\n",
      "loss: 0.816190  [25664/60000]\n",
      "loss: 0.770988  [32064/60000]\n",
      "loss: 0.683734  [38464/60000]\n",
      "loss: 0.834292  [44864/60000]\n",
      "loss: 0.897182  [51264/60000]\n",
      "loss: 0.672022  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.8%, Avg loss: 0.469486 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.628552  [   64/60000]\n",
      "loss: 0.445008  [ 6464/60000]\n",
      "loss: 0.499827  [12864/60000]\n",
      "loss: 0.530588  [19264/60000]\n",
      "loss: 0.638685  [25664/60000]\n",
      "loss: 0.531026  [32064/60000]\n",
      "loss: 0.429523  [38464/60000]\n",
      "loss: 0.450180  [44864/60000]\n",
      "loss: 0.531035  [51264/60000]\n",
      "loss: 0.457174  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.6%, Avg loss: 0.325102 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.407555  [   64/60000]\n",
      "loss: 0.331471  [ 6464/60000]\n",
      "loss: 0.424504  [12864/60000]\n",
      "loss: 0.483455  [19264/60000]\n",
      "loss: 0.469321  [25664/60000]\n",
      "loss: 0.416513  [32064/60000]\n",
      "loss: 0.354267  [38464/60000]\n",
      "loss: 0.455392  [44864/60000]\n",
      "loss: 0.499595  [51264/60000]\n",
      "loss: 0.426093  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.5%, Avg loss: 0.239114 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.404783  [   64/60000]\n",
      "loss: 0.311100  [ 6464/60000]\n",
      "loss: 0.245748  [12864/60000]\n",
      "loss: 0.365786  [19264/60000]\n",
      "loss: 0.329978  [25664/60000]\n",
      "loss: 0.388345  [32064/60000]\n",
      "loss: 0.297705  [38464/60000]\n",
      "loss: 0.325070  [44864/60000]\n",
      "loss: 0.452987  [51264/60000]\n",
      "loss: 0.381704  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.9%, Avg loss: 0.218620 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.331755  [   64/60000]\n",
      "loss: 0.204305  [ 6464/60000]\n",
      "loss: 0.292971  [12864/60000]\n",
      "loss: 0.358994  [19264/60000]\n",
      "loss: 0.278311  [25664/60000]\n",
      "loss: 0.396443  [32064/60000]\n",
      "loss: 0.195483  [38464/60000]\n",
      "loss: 0.277443  [44864/60000]\n",
      "loss: 0.359583  [51264/60000]\n",
      "loss: 0.313761  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.5%, Avg loss: 0.249872 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.243919  [   64/60000]\n",
      "loss: 0.176755  [ 6464/60000]\n",
      "loss: 0.277406  [12864/60000]\n",
      "loss: 0.265126  [19264/60000]\n",
      "loss: 0.268166  [25664/60000]\n",
      "loss: 0.259036  [32064/60000]\n",
      "loss: 0.154269  [38464/60000]\n",
      "loss: 0.215391  [44864/60000]\n",
      "loss: 0.378723  [51264/60000]\n",
      "loss: 0.300223  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.252730 \n",
      "\n",
      "Done!\n",
      "lr: 0.01\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.390606  [   64/60000]\n",
      "loss: 1.688674  [ 6464/60000]\n",
      "loss: 0.837320  [12864/60000]\n",
      "loss: 0.675185  [19264/60000]\n",
      "loss: 0.398238  [25664/60000]\n",
      "loss: 0.409078  [32064/60000]\n",
      "loss: 0.218310  [38464/60000]\n",
      "loss: 0.292847  [44864/60000]\n",
      "loss: 0.380604  [51264/60000]\n",
      "loss: 0.193289  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.156346 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.201881  [   64/60000]\n",
      "loss: 0.188834  [ 6464/60000]\n",
      "loss: 0.145091  [12864/60000]\n",
      "loss: 0.188265  [19264/60000]\n",
      "loss: 0.137702  [25664/60000]\n",
      "loss: 0.183652  [32064/60000]\n",
      "loss: 0.103145  [38464/60000]\n",
      "loss: 0.198003  [44864/60000]\n",
      "loss: 0.272744  [51264/60000]\n",
      "loss: 0.204311  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.2%, Avg loss: 0.094610 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.117032  [   64/60000]\n",
      "loss: 0.128144  [ 6464/60000]\n",
      "loss: 0.113263  [12864/60000]\n",
      "loss: 0.209804  [19264/60000]\n",
      "loss: 0.086546  [25664/60000]\n",
      "loss: 0.178970  [32064/60000]\n",
      "loss: 0.108935  [38464/60000]\n",
      "loss: 0.109865  [44864/60000]\n",
      "loss: 0.307224  [51264/60000]\n",
      "loss: 0.286319  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.5%, Avg loss: 0.112675 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.105556  [   64/60000]\n",
      "loss: 0.191515  [ 6464/60000]\n",
      "loss: 0.149334  [12864/60000]\n",
      "loss: 0.177021  [19264/60000]\n",
      "loss: 0.064074  [25664/60000]\n",
      "loss: 0.105665  [32064/60000]\n",
      "loss: 0.054132  [38464/60000]\n",
      "loss: 0.147437  [44864/60000]\n",
      "loss: 0.248146  [51264/60000]\n",
      "loss: 0.076085  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.4%, Avg loss: 0.110299 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.100617  [   64/60000]\n",
      "loss: 0.201193  [ 6464/60000]\n",
      "loss: 0.111249  [12864/60000]\n",
      "loss: 0.124064  [19264/60000]\n",
      "loss: 0.038128  [25664/60000]\n",
      "loss: 0.148586  [32064/60000]\n",
      "loss: 0.045923  [38464/60000]\n",
      "loss: 0.137252  [44864/60000]\n",
      "loss: 0.281002  [51264/60000]\n",
      "loss: 0.069380  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.7%, Avg loss: 0.074269 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.102713  [   64/60000]\n",
      "loss: 0.136942  [ 6464/60000]\n",
      "loss: 0.122367  [12864/60000]\n",
      "loss: 0.159148  [19264/60000]\n",
      "loss: 0.130374  [25664/60000]\n",
      "loss: 0.109658  [32064/60000]\n",
      "loss: 0.058543  [38464/60000]\n",
      "loss: 0.084063  [44864/60000]\n",
      "loss: 0.186134  [51264/60000]\n",
      "loss: 0.247759  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.056114 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.073668  [   64/60000]\n",
      "loss: 0.129836  [ 6464/60000]\n",
      "loss: 0.112515  [12864/60000]\n",
      "loss: 0.072945  [19264/60000]\n",
      "loss: 0.118485  [25664/60000]\n",
      "loss: 0.082046  [32064/60000]\n",
      "loss: 0.037670  [38464/60000]\n",
      "loss: 0.078092  [44864/60000]\n",
      "loss: 0.194050  [51264/60000]\n",
      "loss: 0.058079  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.4%, Avg loss: 0.049336 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.064110  [   64/60000]\n",
      "loss: 0.162385  [ 6464/60000]\n",
      "loss: 0.147272  [12864/60000]\n",
      "loss: 0.091687  [19264/60000]\n",
      "loss: 0.031827  [25664/60000]\n",
      "loss: 0.029818  [32064/60000]\n",
      "loss: 0.043932  [38464/60000]\n",
      "loss: 0.085483  [44864/60000]\n",
      "loss: 0.219497  [51264/60000]\n",
      "loss: 0.060127  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.050990 \n",
      "\n",
      "Done!\n",
      "lr: 0.1\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.399964  [   64/60000]\n",
      "loss: 0.331305  [ 6464/60000]\n",
      "loss: 0.186006  [12864/60000]\n",
      "loss: 0.276741  [19264/60000]\n",
      "loss: 0.181243  [25664/60000]\n",
      "loss: 0.098057  [32064/60000]\n",
      "loss: 0.060113  [38464/60000]\n",
      "loss: 0.182664  [44864/60000]\n",
      "loss: 0.261129  [51264/60000]\n",
      "loss: 0.144806  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.8%, Avg loss: 0.094955 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.116428  [   64/60000]\n",
      "loss: 0.259644  [ 6464/60000]\n",
      "loss: 0.092016  [12864/60000]\n",
      "loss: 0.201592  [19264/60000]\n",
      "loss: 0.067996  [25664/60000]\n",
      "loss: 0.065239  [32064/60000]\n",
      "loss: 0.050490  [38464/60000]\n",
      "loss: 0.191471  [44864/60000]\n",
      "loss: 0.183744  [51264/60000]\n",
      "loss: 0.083740  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.063132 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.087225  [   64/60000]\n",
      "loss: 0.141589  [ 6464/60000]\n",
      "loss: 0.060587  [12864/60000]\n",
      "loss: 0.096076  [19264/60000]\n",
      "loss: 0.062759  [25664/60000]\n",
      "loss: 0.043685  [32064/60000]\n",
      "loss: 0.143849  [38464/60000]\n",
      "loss: 0.113736  [44864/60000]\n",
      "loss: 0.133611  [51264/60000]\n",
      "loss: 0.042866  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.5%, Avg loss: 0.044905 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.108378  [   64/60000]\n",
      "loss: 0.115653  [ 6464/60000]\n",
      "loss: 0.018921  [12864/60000]\n",
      "loss: 0.185695  [19264/60000]\n",
      "loss: 0.040008  [25664/60000]\n",
      "loss: 0.066617  [32064/60000]\n",
      "loss: 0.071031  [38464/60000]\n",
      "loss: 0.059294  [44864/60000]\n",
      "loss: 0.294046  [51264/60000]\n",
      "loss: 0.030196  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.042657 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.047905  [   64/60000]\n",
      "loss: 0.065573  [ 6464/60000]\n",
      "loss: 0.059093  [12864/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.083470  [19264/60000]\n",
      "loss: 0.024124  [25664/60000]\n",
      "loss: 0.022186  [32064/60000]\n",
      "loss: 0.071796  [38464/60000]\n",
      "loss: 0.090214  [44864/60000]\n",
      "loss: 0.321476  [51264/60000]\n",
      "loss: 0.028896  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.036939 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.015509  [   64/60000]\n",
      "loss: 0.026405  [ 6464/60000]\n",
      "loss: 0.034581  [12864/60000]\n",
      "loss: 0.141381  [19264/60000]\n",
      "loss: 0.038550  [25664/60000]\n",
      "loss: 0.013055  [32064/60000]\n",
      "loss: 0.056221  [38464/60000]\n",
      "loss: 0.026597  [44864/60000]\n",
      "loss: 0.252121  [51264/60000]\n",
      "loss: 0.037837  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.041346 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.015122  [   64/60000]\n",
      "loss: 0.163332  [ 6464/60000]\n",
      "loss: 0.024341  [12864/60000]\n",
      "loss: 0.132303  [19264/60000]\n",
      "loss: 0.018091  [25664/60000]\n",
      "loss: 0.021770  [32064/60000]\n",
      "loss: 0.106586  [38464/60000]\n",
      "loss: 0.053721  [44864/60000]\n",
      "loss: 0.087225  [51264/60000]\n",
      "loss: 0.035212  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.034061 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.033397  [   64/60000]\n",
      "loss: 0.061765  [ 6464/60000]\n",
      "loss: 0.038642  [12864/60000]\n",
      "loss: 0.092920  [19264/60000]\n",
      "loss: 0.009401  [25664/60000]\n",
      "loss: 0.023700  [32064/60000]\n",
      "loss: 0.027879  [38464/60000]\n",
      "loss: 0.020210  [44864/60000]\n",
      "loss: 0.068629  [51264/60000]\n",
      "loss: 0.015669  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.030733 \n",
      "\n",
      "Done!\n",
      "lr: 1\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.418073  [   64/60000]\n",
      "loss: 2.296324  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.303841  [   64/60000]\n",
      "loss: 2.296340  [ 6464/60000]\n",
      "loss: 2.318979  [12864/60000]\n",
      "loss: 2.313561  [19264/60000]\n",
      "loss: 2.314356  [25664/60000]\n",
      "loss: 2.311189  [32064/60000]\n",
      "loss: 2.303037  [38464/60000]\n",
      "loss: 2.305269  [44864/60000]\n",
      "loss: 2.311202  [51264/60000]\n",
      "loss: 2.287770  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 9.6%, Avg loss: 2.302241 \n",
      "\n",
      "Done!\n",
      "lr: 10\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.345338  [   64/60000]\n",
      "loss: 2.352186  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.412003  [   64/60000]\n",
      "loss: 2.352185  [ 6464/60000]\n",
      "loss: 2.419976  [12864/60000]\n",
      "loss: 2.403373  [19264/60000]\n",
      "loss: 2.405704  [25664/60000]\n",
      "loss: 2.313789  [32064/60000]\n",
      "loss: 2.355367  [38464/60000]\n",
      "loss: 2.271533  [44864/60000]\n",
      "loss: 2.401535  [51264/60000]\n",
      "loss: 2.452326  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.3%, Avg loss: 2.380995 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同lr的影响\n",
    "lrList = [0.001,0.01,0.1,1,10]\n",
    "lr_loss = []\n",
    "\n",
    "for lr in lrList:\n",
    "    model = CNN_1().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)#定义优化器\n",
    "    print(f\"lr: {lr}\\n-------------------------------------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    lr_loss.append(test_loss)\n",
    "    print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afbab952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barch: 16\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.458938  [   16/60000]\n",
      "loss: 2.305448  [ 1616/60000]\n",
      "loss: 2.234544  [ 3216/60000]\n",
      "loss: 2.195567  [ 4816/60000]\n",
      "loss: 2.107365  [ 6416/60000]\n",
      "loss: 2.155208  [ 8016/60000]\n",
      "loss: 2.068627  [ 9616/60000]\n",
      "loss: 2.137973  [11216/60000]\n",
      "loss: 1.860306  [12816/60000]\n",
      "loss: 1.736449  [14416/60000]\n",
      "loss: 1.710513  [16016/60000]\n",
      "loss: 1.821875  [17616/60000]\n",
      "loss: 1.737623  [19216/60000]\n",
      "loss: 1.396893  [20816/60000]\n",
      "loss: 1.355071  [22416/60000]\n",
      "loss: 1.126924  [24016/60000]\n",
      "loss: 1.167807  [25616/60000]\n",
      "loss: 0.999632  [27216/60000]\n",
      "loss: 0.864241  [28816/60000]\n",
      "loss: 1.063081  [30416/60000]\n",
      "loss: 0.866053  [32016/60000]\n",
      "loss: 1.104217  [33616/60000]\n",
      "loss: 0.618154  [35216/60000]\n",
      "loss: 1.381436  [36816/60000]\n",
      "loss: 0.812713  [38416/60000]\n",
      "loss: 0.874165  [40016/60000]\n",
      "loss: 0.882425  [41616/60000]\n",
      "loss: 0.767802  [43216/60000]\n",
      "loss: 0.519121  [44816/60000]\n",
      "loss: 0.702675  [46416/60000]\n",
      "loss: 0.763660  [48016/60000]\n",
      "loss: 0.343481  [49616/60000]\n",
      "loss: 0.425369  [51216/60000]\n",
      "loss: 0.963489  [52816/60000]\n",
      "loss: 0.445971  [54416/60000]\n",
      "loss: 0.491073  [56016/60000]\n",
      "loss: 0.344455  [57616/60000]\n",
      "loss: 0.290040  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.8%, Avg loss: 0.400435 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.219722  [   16/60000]\n",
      "loss: 0.732301  [ 1616/60000]\n",
      "loss: 0.487900  [ 3216/60000]\n",
      "loss: 0.521745  [ 4816/60000]\n",
      "loss: 0.320150  [ 6416/60000]\n",
      "loss: 0.593375  [ 8016/60000]\n",
      "loss: 0.636331  [ 9616/60000]\n",
      "loss: 1.187826  [11216/60000]\n",
      "loss: 0.330097  [12816/60000]\n",
      "loss: 0.362404  [14416/60000]\n",
      "loss: 0.366613  [16016/60000]\n",
      "loss: 0.360328  [17616/60000]\n",
      "loss: 0.629966  [19216/60000]\n",
      "loss: 0.269772  [20816/60000]\n",
      "loss: 0.394524  [22416/60000]\n",
      "loss: 0.349105  [24016/60000]\n",
      "loss: 0.393205  [25616/60000]\n",
      "loss: 0.397538  [27216/60000]\n",
      "loss: 0.217250  [28816/60000]\n",
      "loss: 0.288946  [30416/60000]\n",
      "loss: 0.108203  [32016/60000]\n",
      "loss: 0.291554  [33616/60000]\n",
      "loss: 0.286926  [35216/60000]\n",
      "loss: 0.604921  [36816/60000]\n",
      "loss: 0.440414  [38416/60000]\n",
      "loss: 0.281156  [40016/60000]\n",
      "loss: 0.590075  [41616/60000]\n",
      "loss: 0.354785  [43216/60000]\n",
      "loss: 0.424093  [44816/60000]\n",
      "loss: 0.553872  [46416/60000]\n",
      "loss: 0.487759  [48016/60000]\n",
      "loss: 0.238835  [49616/60000]\n",
      "loss: 0.164977  [51216/60000]\n",
      "loss: 0.719485  [52816/60000]\n",
      "loss: 0.108692  [54416/60000]\n",
      "loss: 0.183254  [56016/60000]\n",
      "loss: 0.193926  [57616/60000]\n",
      "loss: 0.087028  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.176542 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.066430  [   16/60000]\n",
      "loss: 0.871071  [ 1616/60000]\n",
      "loss: 0.175429  [ 3216/60000]\n",
      "loss: 0.151120  [ 4816/60000]\n",
      "loss: 0.133682  [ 6416/60000]\n",
      "loss: 0.314530  [ 8016/60000]\n",
      "loss: 0.376004  [ 9616/60000]\n",
      "loss: 0.678802  [11216/60000]\n",
      "loss: 0.078642  [12816/60000]\n",
      "loss: 0.119580  [14416/60000]\n",
      "loss: 0.138947  [16016/60000]\n",
      "loss: 0.262903  [17616/60000]\n",
      "loss: 0.148417  [19216/60000]\n",
      "loss: 0.093133  [20816/60000]\n",
      "loss: 0.350949  [22416/60000]\n",
      "loss: 0.149212  [24016/60000]\n",
      "loss: 0.228384  [25616/60000]\n",
      "loss: 0.254771  [27216/60000]\n",
      "loss: 0.374191  [28816/60000]\n",
      "loss: 0.214513  [30416/60000]\n",
      "loss: 0.131116  [32016/60000]\n",
      "loss: 0.198098  [33616/60000]\n",
      "loss: 0.259071  [35216/60000]\n",
      "loss: 0.197216  [36816/60000]\n",
      "loss: 0.171252  [38416/60000]\n",
      "loss: 0.166738  [40016/60000]\n",
      "loss: 0.428682  [41616/60000]\n",
      "loss: 0.232475  [43216/60000]\n",
      "loss: 0.131405  [44816/60000]\n",
      "loss: 0.418603  [46416/60000]\n",
      "loss: 0.324053  [48016/60000]\n",
      "loss: 0.054479  [49616/60000]\n",
      "loss: 0.088975  [51216/60000]\n",
      "loss: 0.558967  [52816/60000]\n",
      "loss: 0.062796  [54416/60000]\n",
      "loss: 0.213011  [56016/60000]\n",
      "loss: 0.284449  [57616/60000]\n",
      "loss: 0.023649  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.2%, Avg loss: 0.126801 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.018069  [   16/60000]\n",
      "loss: 0.490080  [ 1616/60000]\n",
      "loss: 0.119034  [ 3216/60000]\n",
      "loss: 0.119932  [ 4816/60000]\n",
      "loss: 0.046751  [ 6416/60000]\n",
      "loss: 0.335784  [ 8016/60000]\n",
      "loss: 0.398713  [ 9616/60000]\n",
      "loss: 0.188621  [11216/60000]\n",
      "loss: 0.071172  [12816/60000]\n",
      "loss: 0.125377  [14416/60000]\n",
      "loss: 0.068594  [16016/60000]\n",
      "loss: 0.104970  [17616/60000]\n",
      "loss: 0.057212  [19216/60000]\n",
      "loss: 0.050935  [20816/60000]\n",
      "loss: 0.184500  [22416/60000]\n",
      "loss: 0.138731  [24016/60000]\n",
      "loss: 0.081322  [25616/60000]\n",
      "loss: 0.127080  [27216/60000]\n",
      "loss: 0.096209  [28816/60000]\n",
      "loss: 0.100071  [30416/60000]\n",
      "loss: 0.104133  [32016/60000]\n",
      "loss: 0.257241  [33616/60000]\n",
      "loss: 0.221032  [35216/60000]\n",
      "loss: 0.517939  [36816/60000]\n",
      "loss: 0.129506  [38416/60000]\n",
      "loss: 0.144443  [40016/60000]\n",
      "loss: 0.645070  [41616/60000]\n",
      "loss: 0.188475  [43216/60000]\n",
      "loss: 0.258665  [44816/60000]\n",
      "loss: 0.444473  [46416/60000]\n",
      "loss: 0.348333  [48016/60000]\n",
      "loss: 0.092696  [49616/60000]\n",
      "loss: 0.046972  [51216/60000]\n",
      "loss: 0.675451  [52816/60000]\n",
      "loss: 0.090459  [54416/60000]\n",
      "loss: 0.284951  [56016/60000]\n",
      "loss: 0.149491  [57616/60000]\n",
      "loss: 0.158453  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.9%, Avg loss: 0.099246 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.017663  [   16/60000]\n",
      "loss: 0.788336  [ 1616/60000]\n",
      "loss: 0.143966  [ 3216/60000]\n",
      "loss: 0.077386  [ 4816/60000]\n",
      "loss: 0.014890  [ 6416/60000]\n",
      "loss: 0.132773  [ 8016/60000]\n",
      "loss: 0.230116  [ 9616/60000]\n",
      "loss: 0.627564  [11216/60000]\n",
      "loss: 0.044370  [12816/60000]\n",
      "loss: 0.176363  [14416/60000]\n",
      "loss: 0.044615  [16016/60000]\n",
      "loss: 0.104295  [17616/60000]\n",
      "loss: 0.074729  [19216/60000]\n",
      "loss: 0.119416  [20816/60000]\n",
      "loss: 0.080954  [22416/60000]\n",
      "loss: 0.083104  [24016/60000]\n",
      "loss: 0.157566  [25616/60000]\n",
      "loss: 0.086137  [27216/60000]\n",
      "loss: 0.085509  [28816/60000]\n",
      "loss: 0.101115  [30416/60000]\n",
      "loss: 0.111949  [32016/60000]\n",
      "loss: 0.250281  [33616/60000]\n",
      "loss: 0.119789  [35216/60000]\n",
      "loss: 0.294163  [36816/60000]\n",
      "loss: 0.154281  [38416/60000]\n",
      "loss: 0.172185  [40016/60000]\n",
      "loss: 0.326273  [41616/60000]\n",
      "loss: 0.146467  [43216/60000]\n",
      "loss: 0.071136  [44816/60000]\n",
      "loss: 0.172697  [46416/60000]\n",
      "loss: 0.202209  [48016/60000]\n",
      "loss: 0.099274  [49616/60000]\n",
      "loss: 0.089195  [51216/60000]\n",
      "loss: 0.633442  [52816/60000]\n",
      "loss: 0.023929  [54416/60000]\n",
      "loss: 0.205957  [56016/60000]\n",
      "loss: 0.064134  [57616/60000]\n",
      "loss: 0.013820  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.5%, Avg loss: 0.082475 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.007989  [   16/60000]\n",
      "loss: 0.729116  [ 1616/60000]\n",
      "loss: 0.080276  [ 3216/60000]\n",
      "loss: 0.012679  [ 4816/60000]\n",
      "loss: 0.021738  [ 6416/60000]\n",
      "loss: 0.205356  [ 8016/60000]\n",
      "loss: 0.219151  [ 9616/60000]\n",
      "loss: 0.361732  [11216/60000]\n",
      "loss: 0.077175  [12816/60000]\n",
      "loss: 0.076234  [14416/60000]\n",
      "loss: 0.038763  [16016/60000]\n",
      "loss: 0.119067  [17616/60000]\n",
      "loss: 0.116218  [19216/60000]\n",
      "loss: 0.015683  [20816/60000]\n",
      "loss: 0.057779  [22416/60000]\n",
      "loss: 0.121385  [24016/60000]\n",
      "loss: 0.137306  [25616/60000]\n",
      "loss: 0.021113  [27216/60000]\n",
      "loss: 0.225789  [28816/60000]\n",
      "loss: 0.183021  [30416/60000]\n",
      "loss: 0.032894  [32016/60000]\n",
      "loss: 0.187848  [33616/60000]\n",
      "loss: 0.079629  [35216/60000]\n",
      "loss: 0.143835  [36816/60000]\n",
      "loss: 0.086090  [38416/60000]\n",
      "loss: 0.234556  [40016/60000]\n",
      "loss: 0.201608  [41616/60000]\n",
      "loss: 0.190943  [43216/60000]\n",
      "loss: 0.079135  [44816/60000]\n",
      "loss: 0.257177  [46416/60000]\n",
      "loss: 0.089724  [48016/60000]\n",
      "loss: 0.062712  [49616/60000]\n",
      "loss: 0.034170  [51216/60000]\n",
      "loss: 0.611340  [52816/60000]\n",
      "loss: 0.058885  [54416/60000]\n",
      "loss: 0.147662  [56016/60000]\n",
      "loss: 0.029996  [57616/60000]\n",
      "loss: 0.058359  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.073583 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.157479  [   16/60000]\n",
      "loss: 0.571889  [ 1616/60000]\n",
      "loss: 0.046893  [ 3216/60000]\n",
      "loss: 0.017800  [ 4816/60000]\n",
      "loss: 0.020409  [ 6416/60000]\n",
      "loss: 0.117325  [ 8016/60000]\n",
      "loss: 0.115051  [ 9616/60000]\n",
      "loss: 0.203465  [11216/60000]\n",
      "loss: 0.059825  [12816/60000]\n",
      "loss: 0.032652  [14416/60000]\n",
      "loss: 0.020933  [16016/60000]\n",
      "loss: 0.084491  [17616/60000]\n",
      "loss: 0.048282  [19216/60000]\n",
      "loss: 0.026769  [20816/60000]\n",
      "loss: 0.239500  [22416/60000]\n",
      "loss: 0.086829  [24016/60000]\n",
      "loss: 0.024461  [25616/60000]\n",
      "loss: 0.052398  [27216/60000]\n",
      "loss: 0.055916  [28816/60000]\n",
      "loss: 0.130278  [30416/60000]\n",
      "loss: 0.096301  [32016/60000]\n",
      "loss: 0.100573  [33616/60000]\n",
      "loss: 0.050223  [35216/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.091088  [36816/60000]\n",
      "loss: 0.131799  [38416/60000]\n",
      "loss: 0.282098  [40016/60000]\n",
      "loss: 0.251074  [41616/60000]\n",
      "loss: 0.122868  [43216/60000]\n",
      "loss: 0.081682  [44816/60000]\n",
      "loss: 0.301016  [46416/60000]\n",
      "loss: 0.156486  [48016/60000]\n",
      "loss: 0.033435  [49616/60000]\n",
      "loss: 0.053159  [51216/60000]\n",
      "loss: 0.562637  [52816/60000]\n",
      "loss: 0.040219  [54416/60000]\n",
      "loss: 0.125971  [56016/60000]\n",
      "loss: 0.182416  [57616/60000]\n",
      "loss: 0.023976  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.9%, Avg loss: 0.067584 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.007233  [   16/60000]\n",
      "loss: 0.411373  [ 1616/60000]\n",
      "loss: 0.040364  [ 3216/60000]\n",
      "loss: 0.038198  [ 4816/60000]\n",
      "loss: 0.023284  [ 6416/60000]\n",
      "loss: 0.374255  [ 8016/60000]\n",
      "loss: 0.079322  [ 9616/60000]\n",
      "loss: 0.327499  [11216/60000]\n",
      "loss: 0.024620  [12816/60000]\n",
      "loss: 0.078672  [14416/60000]\n",
      "loss: 0.022489  [16016/60000]\n",
      "loss: 0.052796  [17616/60000]\n",
      "loss: 0.085221  [19216/60000]\n",
      "loss: 0.047136  [20816/60000]\n",
      "loss: 0.065047  [22416/60000]\n",
      "loss: 0.079961  [24016/60000]\n",
      "loss: 0.028762  [25616/60000]\n",
      "loss: 0.035962  [27216/60000]\n",
      "loss: 0.110661  [28816/60000]\n",
      "loss: 0.109492  [30416/60000]\n",
      "loss: 0.053703  [32016/60000]\n",
      "loss: 0.217740  [33616/60000]\n",
      "loss: 0.122498  [35216/60000]\n",
      "loss: 0.135535  [36816/60000]\n",
      "loss: 0.171125  [38416/60000]\n",
      "loss: 0.363195  [40016/60000]\n",
      "loss: 0.393514  [41616/60000]\n",
      "loss: 0.150374  [43216/60000]\n",
      "loss: 0.027049  [44816/60000]\n",
      "loss: 0.083223  [46416/60000]\n",
      "loss: 0.066465  [48016/60000]\n",
      "loss: 0.021674  [49616/60000]\n",
      "loss: 0.064892  [51216/60000]\n",
      "loss: 0.528974  [52816/60000]\n",
      "loss: 0.021448  [54416/60000]\n",
      "loss: 0.161052  [56016/60000]\n",
      "loss: 0.081009  [57616/60000]\n",
      "loss: 0.016360  [59216/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.063354 \n",
      "\n",
      "Done!\n",
      "barch: 32\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.346601  [   32/60000]\n",
      "loss: 2.341022  [ 3232/60000]\n",
      "loss: 2.251538  [ 6432/60000]\n",
      "loss: 2.213666  [ 9632/60000]\n",
      "loss: 2.185805  [12832/60000]\n",
      "loss: 2.048555  [16032/60000]\n",
      "loss: 1.964740  [19232/60000]\n",
      "loss: 1.865693  [22432/60000]\n",
      "loss: 1.755226  [25632/60000]\n",
      "loss: 1.611753  [28832/60000]\n",
      "loss: 1.714249  [32032/60000]\n",
      "loss: 1.511347  [35232/60000]\n",
      "loss: 1.323116  [38432/60000]\n",
      "loss: 1.415501  [41632/60000]\n",
      "loss: 1.133657  [44832/60000]\n",
      "loss: 1.142988  [48032/60000]\n",
      "loss: 1.107409  [51232/60000]\n",
      "loss: 0.884546  [54432/60000]\n",
      "loss: 0.868979  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.824093 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.019168  [   32/60000]\n",
      "loss: 0.898109  [ 3232/60000]\n",
      "loss: 1.112723  [ 6432/60000]\n",
      "loss: 0.923752  [ 9632/60000]\n",
      "loss: 0.802611  [12832/60000]\n",
      "loss: 0.906535  [16032/60000]\n",
      "loss: 0.669040  [19232/60000]\n",
      "loss: 0.736779  [22432/60000]\n",
      "loss: 0.586430  [25632/60000]\n",
      "loss: 0.709282  [28832/60000]\n",
      "loss: 0.717585  [32032/60000]\n",
      "loss: 0.462704  [35232/60000]\n",
      "loss: 0.698971  [38432/60000]\n",
      "loss: 0.778530  [41632/60000]\n",
      "loss: 0.403159  [44832/60000]\n",
      "loss: 0.650686  [48032/60000]\n",
      "loss: 0.552532  [51232/60000]\n",
      "loss: 0.474710  [54432/60000]\n",
      "loss: 0.545695  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.4%, Avg loss: 0.393029 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.486929  [   32/60000]\n",
      "loss: 0.526268  [ 3232/60000]\n",
      "loss: 0.659153  [ 6432/60000]\n",
      "loss: 0.633604  [ 9632/60000]\n",
      "loss: 0.416989  [12832/60000]\n",
      "loss: 0.476487  [16032/60000]\n",
      "loss: 0.620107  [19232/60000]\n",
      "loss: 0.305021  [22432/60000]\n",
      "loss: 0.363583  [25632/60000]\n",
      "loss: 0.460108  [28832/60000]\n",
      "loss: 0.637917  [32032/60000]\n",
      "loss: 0.328769  [35232/60000]\n",
      "loss: 0.463309  [38432/60000]\n",
      "loss: 0.539506  [41632/60000]\n",
      "loss: 0.267014  [44832/60000]\n",
      "loss: 0.405983  [48032/60000]\n",
      "loss: 0.386072  [51232/60000]\n",
      "loss: 0.249468  [54432/60000]\n",
      "loss: 0.372461  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.0%, Avg loss: 0.261312 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.310583  [   32/60000]\n",
      "loss: 0.364723  [ 3232/60000]\n",
      "loss: 0.441444  [ 6432/60000]\n",
      "loss: 0.463861  [ 9632/60000]\n",
      "loss: 0.256253  [12832/60000]\n",
      "loss: 0.511485  [16032/60000]\n",
      "loss: 0.408500  [19232/60000]\n",
      "loss: 0.288156  [22432/60000]\n",
      "loss: 0.326727  [25632/60000]\n",
      "loss: 0.310426  [28832/60000]\n",
      "loss: 0.587816  [32032/60000]\n",
      "loss: 0.203407  [35232/60000]\n",
      "loss: 0.328978  [38432/60000]\n",
      "loss: 0.465976  [41632/60000]\n",
      "loss: 0.299816  [44832/60000]\n",
      "loss: 0.314227  [48032/60000]\n",
      "loss: 0.329133  [51232/60000]\n",
      "loss: 0.225145  [54432/60000]\n",
      "loss: 0.327032  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.3%, Avg loss: 0.202055 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.260466  [   32/60000]\n",
      "loss: 0.304649  [ 3232/60000]\n",
      "loss: 0.359864  [ 6432/60000]\n",
      "loss: 0.360694  [ 9632/60000]\n",
      "loss: 0.199583  [12832/60000]\n",
      "loss: 0.403903  [16032/60000]\n",
      "loss: 0.293627  [19232/60000]\n",
      "loss: 0.186786  [22432/60000]\n",
      "loss: 0.207989  [25632/60000]\n",
      "loss: 0.300061  [28832/60000]\n",
      "loss: 0.561248  [32032/60000]\n",
      "loss: 0.122701  [35232/60000]\n",
      "loss: 0.232660  [38432/60000]\n",
      "loss: 0.380701  [41632/60000]\n",
      "loss: 0.109160  [44832/60000]\n",
      "loss: 0.287065  [48032/60000]\n",
      "loss: 0.252106  [51232/60000]\n",
      "loss: 0.139964  [54432/60000]\n",
      "loss: 0.302174  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.3%, Avg loss: 0.164958 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.216920  [   32/60000]\n",
      "loss: 0.433171  [ 3232/60000]\n",
      "loss: 0.324410  [ 6432/60000]\n",
      "loss: 0.283829  [ 9632/60000]\n",
      "loss: 0.199155  [12832/60000]\n",
      "loss: 0.300766  [16032/60000]\n",
      "loss: 0.141309  [19232/60000]\n",
      "loss: 0.135342  [22432/60000]\n",
      "loss: 0.263348  [25632/60000]\n",
      "loss: 0.253375  [28832/60000]\n",
      "loss: 0.339332  [32032/60000]\n",
      "loss: 0.208887  [35232/60000]\n",
      "loss: 0.223119  [38432/60000]\n",
      "loss: 0.456560  [41632/60000]\n",
      "loss: 0.175604  [44832/60000]\n",
      "loss: 0.278570  [48032/60000]\n",
      "loss: 0.270021  [51232/60000]\n",
      "loss: 0.080408  [54432/60000]\n",
      "loss: 0.190239  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.8%, Avg loss: 0.145370 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.166988  [   32/60000]\n",
      "loss: 0.257504  [ 3232/60000]\n",
      "loss: 0.386555  [ 6432/60000]\n",
      "loss: 0.242151  [ 9632/60000]\n",
      "loss: 0.079059  [12832/60000]\n",
      "loss: 0.386326  [16032/60000]\n",
      "loss: 0.147993  [19232/60000]\n",
      "loss: 0.231327  [22432/60000]\n",
      "loss: 0.194776  [25632/60000]\n",
      "loss: 0.234341  [28832/60000]\n",
      "loss: 0.333664  [32032/60000]\n",
      "loss: 0.124336  [35232/60000]\n",
      "loss: 0.283102  [38432/60000]\n",
      "loss: 0.240326  [41632/60000]\n",
      "loss: 0.075646  [44832/60000]\n",
      "loss: 0.398104  [48032/60000]\n",
      "loss: 0.211787  [51232/60000]\n",
      "loss: 0.080003  [54432/60000]\n",
      "loss: 0.294645  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.3%, Avg loss: 0.125546 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.156824  [   32/60000]\n",
      "loss: 0.284676  [ 3232/60000]\n",
      "loss: 0.155059  [ 6432/60000]\n",
      "loss: 0.204743  [ 9632/60000]\n",
      "loss: 0.044632  [12832/60000]\n",
      "loss: 0.251506  [16032/60000]\n",
      "loss: 0.106105  [19232/60000]\n",
      "loss: 0.123472  [22432/60000]\n",
      "loss: 0.194019  [25632/60000]\n",
      "loss: 0.244539  [28832/60000]\n",
      "loss: 0.286142  [32032/60000]\n",
      "loss: 0.158456  [35232/60000]\n",
      "loss: 0.220072  [38432/60000]\n",
      "loss: 0.319465  [41632/60000]\n",
      "loss: 0.103808  [44832/60000]\n",
      "loss: 0.212695  [48032/60000]\n",
      "loss: 0.175791  [51232/60000]\n",
      "loss: 0.069591  [54432/60000]\n",
      "loss: 0.180988  [57632/60000]\n",
      "Test Error: \n",
      " Accuracy: 96.6%, Avg loss: 0.114684 \n",
      "\n",
      "Done!\n",
      "barch: 64\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.393382  [   64/60000]\n",
      "loss: 2.264237  [ 6464/60000]\n",
      "loss: 2.293652  [12864/60000]\n",
      "loss: 2.129473  [19264/60000]\n",
      "loss: 2.141728  [25664/60000]\n",
      "loss: 2.162825  [32064/60000]\n",
      "loss: 1.943832  [38464/60000]\n",
      "loss: 1.932568  [44864/60000]\n",
      "loss: 1.918904  [51264/60000]\n",
      "loss: 1.790043  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 1.696302 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.822759  [   64/60000]\n",
      "loss: 1.670680  [ 6464/60000]\n",
      "loss: 1.583510  [12864/60000]\n",
      "loss: 1.458812  [19264/60000]\n",
      "loss: 1.430953  [25664/60000]\n",
      "loss: 1.312880  [32064/60000]\n",
      "loss: 1.192541  [38464/60000]\n",
      "loss: 1.146482  [44864/60000]\n",
      "loss: 1.179432  [51264/60000]\n",
      "loss: 1.070651  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.855695 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.974242  [   64/60000]\n",
      "loss: 0.977093  [ 6464/60000]\n",
      "loss: 0.847957  [12864/60000]\n",
      "loss: 0.854189  [19264/60000]\n",
      "loss: 0.770543  [25664/60000]\n",
      "loss: 0.799794  [32064/60000]\n",
      "loss: 0.693304  [38464/60000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.764141  [44864/60000]\n",
      "loss: 0.691098  [51264/60000]\n",
      "loss: 0.805788  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 89.4%, Avg loss: 0.499780 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.722129  [   64/60000]\n",
      "loss: 0.621091  [ 6464/60000]\n",
      "loss: 0.573750  [12864/60000]\n",
      "loss: 0.674524  [19264/60000]\n",
      "loss: 0.609381  [25664/60000]\n",
      "loss: 0.513650  [32064/60000]\n",
      "loss: 0.406509  [38464/60000]\n",
      "loss: 0.674784  [44864/60000]\n",
      "loss: 0.458065  [51264/60000]\n",
      "loss: 0.459487  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.2%, Avg loss: 0.347665 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.457394  [   64/60000]\n",
      "loss: 0.387700  [ 6464/60000]\n",
      "loss: 0.375642  [12864/60000]\n",
      "loss: 0.530944  [19264/60000]\n",
      "loss: 0.379961  [25664/60000]\n",
      "loss: 0.436235  [32064/60000]\n",
      "loss: 0.331166  [38464/60000]\n",
      "loss: 0.489751  [44864/60000]\n",
      "loss: 0.391076  [51264/60000]\n",
      "loss: 0.437654  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.269852 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.417112  [   64/60000]\n",
      "loss: 0.367318  [ 6464/60000]\n",
      "loss: 0.285931  [12864/60000]\n",
      "loss: 0.425646  [19264/60000]\n",
      "loss: 0.352506  [25664/60000]\n",
      "loss: 0.322907  [32064/60000]\n",
      "loss: 0.290640  [38464/60000]\n",
      "loss: 0.272665  [44864/60000]\n",
      "loss: 0.328465  [51264/60000]\n",
      "loss: 0.435056  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.4%, Avg loss: 0.217743 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.310133  [   64/60000]\n",
      "loss: 0.351284  [ 6464/60000]\n",
      "loss: 0.259986  [12864/60000]\n",
      "loss: 0.373315  [19264/60000]\n",
      "loss: 0.274044  [25664/60000]\n",
      "loss: 0.225643  [32064/60000]\n",
      "loss: 0.239163  [38464/60000]\n",
      "loss: 0.342297  [44864/60000]\n",
      "loss: 0.298088  [51264/60000]\n",
      "loss: 0.471154  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.0%, Avg loss: 0.185590 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.253143  [   64/60000]\n",
      "loss: 0.287529  [ 6464/60000]\n",
      "loss: 0.269958  [12864/60000]\n",
      "loss: 0.244739  [19264/60000]\n",
      "loss: 0.255138  [25664/60000]\n",
      "loss: 0.223164  [32064/60000]\n",
      "loss: 0.241749  [38464/60000]\n",
      "loss: 0.240755  [44864/60000]\n",
      "loss: 0.313319  [51264/60000]\n",
      "loss: 0.279833  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.5%, Avg loss: 0.162516 \n",
      "\n",
      "Done!\n",
      "barch: 128\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.409815  [  128/60000]\n",
      "loss: 2.277348  [12928/60000]\n",
      "loss: 2.185420  [25728/60000]\n",
      "loss: 2.112568  [38528/60000]\n",
      "loss: 2.086574  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Avg loss: 2.006058 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.023977  [  128/60000]\n",
      "loss: 1.955238  [12928/60000]\n",
      "loss: 1.850035  [25728/60000]\n",
      "loss: 1.787228  [38528/60000]\n",
      "loss: 1.765727  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 1.579298 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.627130  [  128/60000]\n",
      "loss: 1.571334  [12928/60000]\n",
      "loss: 1.457193  [25728/60000]\n",
      "loss: 1.333977  [38528/60000]\n",
      "loss: 1.377070  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.4%, Avg loss: 1.085185 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.114825  [  128/60000]\n",
      "loss: 1.104702  [12928/60000]\n",
      "loss: 0.986871  [25728/60000]\n",
      "loss: 0.890582  [38528/60000]\n",
      "loss: 1.066064  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.5%, Avg loss: 0.746416 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.832616  [  128/60000]\n",
      "loss: 0.805822  [12928/60000]\n",
      "loss: 0.705150  [25728/60000]\n",
      "loss: 0.708909  [38528/60000]\n",
      "loss: 0.782551  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.540440 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.656475  [  128/60000]\n",
      "loss: 0.616904  [12928/60000]\n",
      "loss: 0.541759  [25728/60000]\n",
      "loss: 0.579464  [38528/60000]\n",
      "loss: 0.668135  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.7%, Avg loss: 0.424694 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.478893  [  128/60000]\n",
      "loss: 0.499713  [12928/60000]\n",
      "loss: 0.468457  [25728/60000]\n",
      "loss: 0.439958  [38528/60000]\n",
      "loss: 0.554934  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 92.9%, Avg loss: 0.347778 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.431415  [  128/60000]\n",
      "loss: 0.503949  [12928/60000]\n",
      "loss: 0.349893  [25728/60000]\n",
      "loss: 0.422902  [38528/60000]\n",
      "loss: 0.552497  [51328/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.7%, Avg loss: 0.292799 \n",
      "\n",
      "Done!\n",
      "barch: 256\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.390651  [  256/60000]\n",
      "loss: 2.287737  [25856/60000]\n",
      "loss: 2.241429  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.182484 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.213375  [  256/60000]\n",
      "loss: 2.142194  [25856/60000]\n",
      "loss: 2.103978  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Avg loss: 2.050051 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.093052  [  256/60000]\n",
      "loss: 2.062480  [25856/60000]\n",
      "loss: 1.950555  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.878328 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.914946  [  256/60000]\n",
      "loss: 1.854250  [25856/60000]\n",
      "loss: 1.772330  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.4%, Avg loss: 1.666112 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.719984  [  256/60000]\n",
      "loss: 1.635455  [25856/60000]\n",
      "loss: 1.544824  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 1.444152 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.516543  [  256/60000]\n",
      "loss: 1.391482  [25856/60000]\n",
      "loss: 1.455913  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 77.0%, Avg loss: 1.233182 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.286265  [  256/60000]\n",
      "loss: 1.205382  [25856/60000]\n",
      "loss: 1.199675  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.2%, Avg loss: 1.052502 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.093664  [  256/60000]\n",
      "loss: 1.065123  [25856/60000]\n",
      "loss: 1.090536  [51456/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.906618 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同batchsize的影响\n",
    "batchSizeList = [16, 32, 64, 128, 256]\n",
    "batch_loss = []\n",
    "\n",
    "for batch in batchSizeList:\n",
    "    model = CNN_1().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "    train_dataloader = DataLoader(training_data, batch_size=batch)\n",
    "    test_dataloader = DataLoader(test_data, batch_size=batch)\n",
    "    print(f\"barch: {batch}\\n-------------------------------------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    batch_loss.append(test_loss)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "639ea212",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: MSELoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.314305  [   64/60000]\n",
      "loss: 0.186617  [ 6464/60000]\n",
      "loss: 0.169233  [12864/60000]\n",
      "loss: 0.141465  [19264/60000]\n",
      "loss: 0.137217  [25664/60000]\n",
      "loss: 0.134567  [32064/60000]\n",
      "loss: 0.133814  [38464/60000]\n",
      "loss: 0.131038  [44864/60000]\n",
      "loss: 0.116091  [51264/60000]\n",
      "loss: 0.106964  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 26.5%, Avg loss: 0.086673 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.119623  [   64/60000]\n",
      "loss: 0.112449  [ 6464/60000]\n",
      "loss: 0.111531  [12864/60000]\n",
      "loss: 0.105565  [19264/60000]\n",
      "loss: 0.102908  [25664/60000]\n",
      "loss: 0.106589  [32064/60000]\n",
      "loss: 0.102198  [38464/60000]\n",
      "loss: 0.103200  [44864/60000]\n",
      "loss: 0.107143  [51264/60000]\n",
      "loss: 0.106082  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 37.4%, Avg loss: 0.082653 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.099312  [   64/60000]\n",
      "loss: 0.102246  [ 6464/60000]\n",
      "loss: 0.091033  [12864/60000]\n",
      "loss: 0.095336  [19264/60000]\n",
      "loss: 0.090533  [25664/60000]\n",
      "loss: 0.099037  [32064/60000]\n",
      "loss: 0.085498  [38464/60000]\n",
      "loss: 0.094066  [44864/60000]\n",
      "loss: 0.094319  [51264/60000]\n",
      "loss: 0.089710  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.7%, Avg loss: 0.080109 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.090308  [   64/60000]\n",
      "loss: 0.089030  [ 6464/60000]\n",
      "loss: 0.089615  [12864/60000]\n",
      "loss: 0.089405  [19264/60000]\n",
      "loss: 0.092343  [25664/60000]\n",
      "loss: 0.090009  [32064/60000]\n",
      "loss: 0.083671  [38464/60000]\n",
      "loss: 0.088358  [44864/60000]\n",
      "loss: 0.090305  [51264/60000]\n",
      "loss: 0.085201  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 51.3%, Avg loss: 0.077865 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.085327  [   64/60000]\n",
      "loss: 0.085218  [ 6464/60000]\n",
      "loss: 0.088293  [12864/60000]\n",
      "loss: 0.083532  [19264/60000]\n",
      "loss: 0.083660  [25664/60000]\n",
      "loss: 0.087491  [32064/60000]\n",
      "loss: 0.079255  [38464/60000]\n",
      "loss: 0.088077  [44864/60000]\n",
      "loss: 0.087338  [51264/60000]\n",
      "loss: 0.085121  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 55.8%, Avg loss: 0.075781 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.083290  [   64/60000]\n",
      "loss: 0.083666  [ 6464/60000]\n",
      "loss: 0.079161  [12864/60000]\n",
      "loss: 0.080629  [19264/60000]\n",
      "loss: 0.087111  [25664/60000]\n",
      "loss: 0.083285  [32064/60000]\n",
      "loss: 0.078218  [38464/60000]\n",
      "loss: 0.084589  [44864/60000]\n",
      "loss: 0.081769  [51264/60000]\n",
      "loss: 0.077726  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 0.073752 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.080353  [   64/60000]\n",
      "loss: 0.078630  [ 6464/60000]\n",
      "loss: 0.081249  [12864/60000]\n",
      "loss: 0.078501  [19264/60000]\n",
      "loss: 0.078103  [25664/60000]\n",
      "loss: 0.079312  [32064/60000]\n",
      "loss: 0.076950  [38464/60000]\n",
      "loss: 0.081357  [44864/60000]\n",
      "loss: 0.082471  [51264/60000]\n",
      "loss: 0.080420  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 0.071739 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.078127  [   64/60000]\n",
      "loss: 0.076649  [ 6464/60000]\n",
      "loss: 0.075795  [12864/60000]\n",
      "loss: 0.075577  [19264/60000]\n",
      "loss: 0.077367  [25664/60000]\n",
      "loss: 0.078754  [32064/60000]\n",
      "loss: 0.074718  [38464/60000]\n",
      "loss: 0.079723  [44864/60000]\n",
      "loss: 0.078032  [51264/60000]\n",
      "loss: 0.079761  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.069712 \n",
      "\n",
      "Done!\n",
      "loss: CrossEntropyLoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.362268  [   64/60000]\n",
      "loss: 2.303295  [ 6464/60000]\n",
      "loss: 2.229652  [12864/60000]\n",
      "loss: 2.166276  [19264/60000]\n",
      "loss: 2.076271  [25664/60000]\n",
      "loss: 2.212629  [32064/60000]\n",
      "loss: 2.053222  [38464/60000]\n",
      "loss: 2.042629  [44864/60000]\n",
      "loss: 2.010971  [51264/60000]\n",
      "loss: 1.930938  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.0%, Avg loss: 1.829203 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.893375  [   64/60000]\n",
      "loss: 1.813634  [ 6464/60000]\n",
      "loss: 1.736059  [12864/60000]\n",
      "loss: 1.626696  [19264/60000]\n",
      "loss: 1.559043  [25664/60000]\n",
      "loss: 1.470476  [32064/60000]\n",
      "loss: 1.325833  [38464/60000]\n",
      "loss: 1.316140  [44864/60000]\n",
      "loss: 1.326289  [51264/60000]\n",
      "loss: 1.170029  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.984129 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.117315  [   64/60000]\n",
      "loss: 0.973469  [ 6464/60000]\n",
      "loss: 0.900271  [12864/60000]\n",
      "loss: 0.926429  [19264/60000]\n",
      "loss: 0.755707  [25664/60000]\n",
      "loss: 0.859679  [32064/60000]\n",
      "loss: 0.723292  [38464/60000]\n",
      "loss: 0.760580  [44864/60000]\n",
      "loss: 0.861227  [51264/60000]\n",
      "loss: 0.702059  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.542830 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.672029  [   64/60000]\n",
      "loss: 0.641608  [ 6464/60000]\n",
      "loss: 0.599998  [12864/60000]\n",
      "loss: 0.634291  [19264/60000]\n",
      "loss: 0.567665  [25664/60000]\n",
      "loss: 0.565148  [32064/60000]\n",
      "loss: 0.547166  [38464/60000]\n",
      "loss: 0.614543  [44864/60000]\n",
      "loss: 0.631791  [51264/60000]\n",
      "loss: 0.555773  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.364466 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.547783  [   64/60000]\n",
      "loss: 0.459657  [ 6464/60000]\n",
      "loss: 0.327298  [12864/60000]\n",
      "loss: 0.603119  [19264/60000]\n",
      "loss: 0.380591  [25664/60000]\n",
      "loss: 0.360514  [32064/60000]\n",
      "loss: 0.369795  [38464/60000]\n",
      "loss: 0.441979  [44864/60000]\n",
      "loss: 0.408834  [51264/60000]\n",
      "loss: 0.327506  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.272019 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.361028  [   64/60000]\n",
      "loss: 0.419468  [ 6464/60000]\n",
      "loss: 0.286891  [12864/60000]\n",
      "loss: 0.540082  [19264/60000]\n",
      "loss: 0.265794  [25664/60000]\n",
      "loss: 0.365489  [32064/60000]\n",
      "loss: 0.267318  [38464/60000]\n",
      "loss: 0.328148  [44864/60000]\n",
      "loss: 0.449243  [51264/60000]\n",
      "loss: 0.359199  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.2%, Avg loss: 0.221778 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.336339  [   64/60000]\n",
      "loss: 0.423337  [ 6464/60000]\n",
      "loss: 0.251820  [12864/60000]\n",
      "loss: 0.305406  [19264/60000]\n",
      "loss: 0.296211  [25664/60000]\n",
      "loss: 0.232579  [32064/60000]\n",
      "loss: 0.210545  [38464/60000]\n",
      "loss: 0.393088  [44864/60000]\n",
      "loss: 0.391970  [51264/60000]\n",
      "loss: 0.267455  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.1%, Avg loss: 0.184762 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.327177  [   64/60000]\n",
      "loss: 0.359198  [ 6464/60000]\n",
      "loss: 0.183087  [12864/60000]\n",
      "loss: 0.274930  [19264/60000]\n",
      "loss: 0.233778  [25664/60000]\n",
      "loss: 0.179670  [32064/60000]\n",
      "loss: 0.220139  [38464/60000]\n",
      "loss: 0.212673  [44864/60000]\n",
      "loss: 0.347575  [51264/60000]\n",
      "loss: 0.266733  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 95.6%, Avg loss: 0.163840 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同损失函数的影响\n",
    "loss_fnList = [nn.MSELoss(), nn.CrossEntropyLoss()]\n",
    "fn_loss = []\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for loss_fn in loss_fnList:\n",
    "    model = CNN_1().to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "    flag = 0\n",
    "    if isinstance(loss_fn, nn.MSELoss):\n",
    "        flag = 1\n",
    "    else:\n",
    "        flag = 0\n",
    "    print(f\"loss: {loss_fn}\\n-------------------------------------------------------------\")\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer, flag = flag)\n",
    "        test_loss = test(test_dataloader, model, loss_fn, flag=flag)\n",
    "    fn_loss.append(test_loss)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aa0e4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: CrossEntropyLoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.462541  [   64/60000]\n",
      "loss: 2.360496  [ 6464/60000]\n",
      "loss: 2.186509  [12864/60000]\n",
      "loss: 2.241232  [19264/60000]\n",
      "loss: 2.211177  [25664/60000]\n",
      "loss: 2.207590  [32064/60000]\n",
      "loss: 1.966873  [38464/60000]\n",
      "loss: 2.038838  [44864/60000]\n",
      "loss: 2.008360  [51264/60000]\n",
      "loss: 1.883597  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.734086 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.834189  [   64/60000]\n",
      "loss: 1.742633  [ 6464/60000]\n",
      "loss: 1.538739  [12864/60000]\n",
      "loss: 1.673459  [19264/60000]\n",
      "loss: 1.570899  [25664/60000]\n",
      "loss: 1.481258  [32064/60000]\n",
      "loss: 1.243336  [38464/60000]\n",
      "loss: 1.284279  [44864/60000]\n",
      "loss: 1.288906  [51264/60000]\n",
      "loss: 1.235968  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.1%, Avg loss: 0.951200 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.139205  [   64/60000]\n",
      "loss: 1.072392  [ 6464/60000]\n",
      "loss: 0.902140  [12864/60000]\n",
      "loss: 1.058379  [19264/60000]\n",
      "loss: 0.913246  [25664/60000]\n",
      "loss: 0.884855  [32064/60000]\n",
      "loss: 0.736873  [38464/60000]\n",
      "loss: 0.748998  [44864/60000]\n",
      "loss: 0.745087  [51264/60000]\n",
      "loss: 0.734190  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.3%, Avg loss: 0.560438 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.728039  [   64/60000]\n",
      "loss: 0.703112  [ 6464/60000]\n",
      "loss: 0.507288  [12864/60000]\n",
      "loss: 0.764148  [19264/60000]\n",
      "loss: 0.624381  [25664/60000]\n",
      "loss: 0.711759  [32064/60000]\n",
      "loss: 0.578648  [38464/60000]\n",
      "loss: 0.570601  [44864/60000]\n",
      "loss: 0.574254  [51264/60000]\n",
      "loss: 0.577512  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.2%, Avg loss: 0.395643 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.626112  [   64/60000]\n",
      "loss: 0.582857  [ 6464/60000]\n",
      "loss: 0.423319  [12864/60000]\n",
      "loss: 0.614910  [19264/60000]\n",
      "loss: 0.508142  [25664/60000]\n",
      "loss: 0.447407  [32064/60000]\n",
      "loss: 0.452276  [38464/60000]\n",
      "loss: 0.421419  [44864/60000]\n",
      "loss: 0.483367  [51264/60000]\n",
      "loss: 0.520404  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.3%, Avg loss: 0.316551 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.441373  [   64/60000]\n",
      "loss: 0.412844  [ 6464/60000]\n",
      "loss: 0.333993  [12864/60000]\n",
      "loss: 0.541872  [19264/60000]\n",
      "loss: 0.489219  [25664/60000]\n",
      "loss: 0.371635  [32064/60000]\n",
      "loss: 0.421401  [38464/60000]\n",
      "loss: 0.363014  [44864/60000]\n",
      "loss: 0.433929  [51264/60000]\n",
      "loss: 0.467573  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.246842 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.376162  [   64/60000]\n",
      "loss: 0.274856  [ 6464/60000]\n",
      "loss: 0.290117  [12864/60000]\n",
      "loss: 0.465269  [19264/60000]\n",
      "loss: 0.332041  [25664/60000]\n",
      "loss: 0.312152  [32064/60000]\n",
      "loss: 0.319897  [38464/60000]\n",
      "loss: 0.284162  [44864/60000]\n",
      "loss: 0.367521  [51264/60000]\n",
      "loss: 0.380334  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.0%, Avg loss: 0.213196 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.466818  [   64/60000]\n",
      "loss: 0.303698  [ 6464/60000]\n",
      "loss: 0.178914  [12864/60000]\n",
      "loss: 0.419280  [19264/60000]\n",
      "loss: 0.246446  [25664/60000]\n",
      "loss: 0.336870  [32064/60000]\n",
      "loss: 0.311841  [38464/60000]\n",
      "loss: 0.241183  [44864/60000]\n",
      "loss: 0.324716  [51264/60000]\n",
      "loss: 0.401784  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.9%, Avg loss: 0.180844 \n",
      "\n",
      "Done!\n",
      "loss: CrossEntropyLoss()\n",
      "-------------------------------------------------------------\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.358476  [   64/60000]\n",
      "loss: 0.268218  [ 6464/60000]\n",
      "loss: 0.131616  [12864/60000]\n",
      "loss: 0.266763  [19264/60000]\n",
      "loss: 0.165058  [25664/60000]\n",
      "loss: 0.113316  [32064/60000]\n",
      "loss: 0.067397  [38464/60000]\n",
      "loss: 0.095406  [44864/60000]\n",
      "loss: 0.240668  [51264/60000]\n",
      "loss: 0.072734  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 97.8%, Avg loss: 0.070010 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.096435  [   64/60000]\n",
      "loss: 0.111877  [ 6464/60000]\n",
      "loss: 0.061469  [12864/60000]\n",
      "loss: 0.124380  [19264/60000]\n",
      "loss: 0.069766  [25664/60000]\n",
      "loss: 0.044486  [32064/60000]\n",
      "loss: 0.082069  [38464/60000]\n",
      "loss: 0.047724  [44864/60000]\n",
      "loss: 0.259197  [51264/60000]\n",
      "loss: 0.077726  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.1%, Avg loss: 0.058536 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.036419  [   64/60000]\n",
      "loss: 0.092417  [ 6464/60000]\n",
      "loss: 0.055566  [12864/60000]\n",
      "loss: 0.049395  [19264/60000]\n",
      "loss: 0.093810  [25664/60000]\n",
      "loss: 0.083308  [32064/60000]\n",
      "loss: 0.079441  [38464/60000]\n",
      "loss: 0.037531  [44864/60000]\n",
      "loss: 0.153611  [51264/60000]\n",
      "loss: 0.030923  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.043050 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.032655  [   64/60000]\n",
      "loss: 0.115784  [ 6464/60000]\n",
      "loss: 0.096792  [12864/60000]\n",
      "loss: 0.107628  [19264/60000]\n",
      "loss: 0.130726  [25664/60000]\n",
      "loss: 0.047942  [32064/60000]\n",
      "loss: 0.043446  [38464/60000]\n",
      "loss: 0.008087  [44864/60000]\n",
      "loss: 0.200464  [51264/60000]\n",
      "loss: 0.032221  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.6%, Avg loss: 0.044347 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.013541  [   64/60000]\n",
      "loss: 0.069144  [ 6464/60000]\n",
      "loss: 0.016031  [12864/60000]\n",
      "loss: 0.028770  [19264/60000]\n",
      "loss: 0.162991  [25664/60000]\n",
      "loss: 0.029324  [32064/60000]\n",
      "loss: 0.014534  [38464/60000]\n",
      "loss: 0.032776  [44864/60000]\n",
      "loss: 0.230404  [51264/60000]\n",
      "loss: 0.044153  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.040764 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.017799  [   64/60000]\n",
      "loss: 0.147367  [ 6464/60000]\n",
      "loss: 0.025356  [12864/60000]\n",
      "loss: 0.044831  [19264/60000]\n",
      "loss: 0.082466  [25664/60000]\n",
      "loss: 0.072553  [32064/60000]\n",
      "loss: 0.014149  [38464/60000]\n",
      "loss: 0.027825  [44864/60000]\n",
      "loss: 0.154912  [51264/60000]\n",
      "loss: 0.019325  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.050631 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.015523  [   64/60000]\n",
      "loss: 0.023214  [ 6464/60000]\n",
      "loss: 0.024382  [12864/60000]\n",
      "loss: 0.044772  [19264/60000]\n",
      "loss: 0.072070  [25664/60000]\n",
      "loss: 0.013953  [32064/60000]\n",
      "loss: 0.046851  [38464/60000]\n",
      "loss: 0.011882  [44864/60000]\n",
      "loss: 0.093265  [51264/60000]\n",
      "loss: 0.040264  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.039152 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.026623  [   64/60000]\n",
      "loss: 0.099844  [ 6464/60000]\n",
      "loss: 0.022171  [12864/60000]\n",
      "loss: 0.089307  [19264/60000]\n",
      "loss: 0.033018  [25664/60000]\n",
      "loss: 0.035674  [32064/60000]\n",
      "loss: 0.011924  [38464/60000]\n",
      "loss: 0.006513  [44864/60000]\n",
      "loss: 0.247149  [51264/60000]\n",
      "loss: 0.047467  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.037594 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# 不同优化器的影响\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "optim_loss = []\n",
    "for i in range(2):\n",
    "    model = CNN_1().to(device)\n",
    "    # 进行优化器选择\n",
    "    if i == 0:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)#定义优化器\n",
    "    if i == 1:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "    print(f\"loss: {loss_fn}\\n-------------------------------------------------------------\")\n",
    "    # 进行训练\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "    optim_loss.append(test_loss)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45c72d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画图操作\n",
    "# 同时进行存储\n",
    "plt.subplot(221)\n",
    "Listlr = list(range(len(lrList)))\n",
    "plt.plot(Listlr, lr_loss, color='r', marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch', fontsize=14, color='r')\n",
    "plt.ylabel('Loss', fontsize=14, color='r')\n",
    "plt.subplot(222)\n",
    "Listbatch = list(range(len(batchSizeList)))\n",
    "plt.plot(Listbatch, batch_loss, color='r', marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch', fontsize=14, color='r')\n",
    "plt.ylabel('Loss', fontsize=14, color='r')\n",
    "plt.subplot(223)\n",
    "ListLoss = list(range(len(loss_fnList)))\n",
    "plt.plot(ListLoss, fn_loss, color='r', marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch', fontsize=14, color='r')\n",
    "plt.ylabel('Loss', fontsize=14, color='r')\n",
    "\n",
    "plt.subplot(224)\n",
    "ListOptim = list(range(2))\n",
    "plt.plot(ListOptim, optim_loss, color='r', marker='o', linestyle='-')\n",
    "plt.xlabel('Epoch', fontsize=14, color='r')\n",
    "plt.ylabel('Loss', fontsize=14, color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57bb66f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model path exists\n",
      "torch.Size([13, 1, 3, 3])\n",
      "0_convlayer shape:(13, 1, 3, 3)\n",
      "torch.Size([10, 13, 3, 3])\n",
      "1_convlayer shape:(10, 13, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "# 显示卷积核\n",
    "# 卷积核可视化\n",
    "# 设置存储路径\n",
    "baseDir = os.path.dirname(\"./\")\n",
    "log_dir = os.path.join(baseDir, \"result\")\n",
    "writer = SummaryWriter(log_dir=log_dir, filename_suffix=\"_kernel\")\n",
    "\n",
    "# 设置权重文件的路径\n",
    "state_dict_path = \"./model.pth\"\n",
    "#判断路径是否存在\n",
    "if os.path.exists(state_dict_path):\n",
    "    # 如果权重文件存在，则把他加载进来\n",
    "    print(\"model path exists\")\n",
    "    visual_model = CNN_1().to(device)\n",
    "    state_dict = torch.load(state_dict_path)\n",
    "    visual_model.load_state_dict(state_dict)\n",
    "else:\n",
    "    print(\"model path dosen exist\")\n",
    "    \n",
    "\n",
    "# 想要访问的起始网络层的序号\n",
    "kernel_num = -1\n",
    "\n",
    "# 最多可视化到的网络层序号\n",
    "vis_max = 1\n",
    "\n",
    "# 遍历网络层\n",
    "for sub_module in visual_model.modules():\n",
    "    # 判断是否为卷积层\n",
    "    if isinstance(sub_module, nn.Conv2d):\n",
    "        kernel_num += 1\n",
    "        # 判断是否超过要可视化的网络层序号\n",
    "        if kernel_num > vis_max:\n",
    "            break\n",
    "        kernels = sub_module.weight\n",
    "        # 得到输出通道数，输入通道数，核宽，核高\n",
    "        c_out, c_int, k_w, k_h = tuple(kernels.shape)\n",
    "        print(kernels.shape)\n",
    "        # 对c_out单独可视化\n",
    "        for o_idx in range(c_out):\n",
    "            kernel_idx = kernels[o_idx, :, :, :].unsqueeze(1)  # make_grid需要 BCHW，这里拓展C维度\n",
    "            \n",
    "            kernel_grid = vutils.make_grid(kernel_idx, normalize=True, scale_each=True, nrow=c_int)\n",
    "            writer.add_image('{}_Convlayer_split_in_channel'.format(kernel_num), kernel_grid)\n",
    "                \n",
    "        # 对64个卷积核直接进行可视化\n",
    "        kernel_all = kernels.view(-1, 1, k_h, k_w)  # b，3, h, w\n",
    "        kernel_grid = vutils.make_grid(kernel_all, normalize=True, scale_each=True, nrow=8)  # c, h, w\n",
    "        writer.add_image('{}_all'.format(kernel_num), kernel_grid, global_step=322)\n",
    "        print(\"{}_convlayer shape:{}\".format(kernel_num, tuple(kernels.shape)))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ca6e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
